
\section{Lattice Basis Reduction} % perhaps move to Algorithms
% For survey see Ngu11, NV10, MW16

% TODO: rewrite (taken from AGVW17)
Problem: usually ugly basis (long vectors...), we want a better basis with shorter and more orthogonal basis vectors...
- improve lattice basis quality => measure by hermite factor (compare shortest vector in basis to lattice volume) or approximation factor (compare shortest vector in basis to shortest lattice vector)
- algorithm finding vector with approximation factor $\gamma$ can be used to solve uSVP with gap $\lambda_2(\Lambda)/\lambda_1(\Lambda) > \gamma $
- best known theoretical bound by Slide reduction \cite{GN08a}, BKZ better in practice

% TODO possibly put Gram-Schmidt orthogonalization here? => Size reduction algorithm and HKZ reduction, see 2.4.1 in Chen13, not sure if needed
- measure quality of basis: Hermite factor  % TODO change or \cite{Reg10}

  * basis $\mathbf{B} = \left\{\mathbf{b}_1, \ldots, \mathbf{b}_m\right\}$, $m$-dimensional lattice $\Lambda(\mathbf{B})$ has root Hermite factor $\delta$ if
  \begin{equation} \label{eq:hermite}
    \| \mathbf{b}_1 \| \approx \delta^m \det(\Lambda)^{1/m}
  \end{equation}

  * use Geometric Series Assumption (GSA) \cite{Sch03} to obtain estimates for $\mathbf{b}_i$: % TODO: necessary? 
    \begin{equation} \label{eq:GSA}
      \| \tilde{\mathbf{b}}_i \| \approx \alpha^{i-1} \| \mathbf{b}_1 \|
    \end{equation}
    for $0 < \alpha < 1$
    \cref{eq:hermite} into \cref{eq:GSA} -> $\| \tilde{\mathbf{b}}_i \| \approx \alpha^{i-1} \delta^m \det(\Lambda)^{1/m}$
    with $\prod_{i-1}^m \| \tilde{\mathbf{b}}_i \| = \det(\Lambda)$ we get 
    \begin{align*}
      &\quad& \prod_{i-1}^m \| \tilde{\mathbf{b}}_i \| &\approx \prod_{i-1}^m \alpha^{i-1} \delta^m \det(\Lambda)^{1/m} \\
      \iff&\quad& \det(\Lambda) &\approx \delta^{2m} \det(\Lambda) \prod_{i-1}^m \alpha^{i-1}\\
      \iff&\quad& \delta^{-m^2}  &\approx \alpha^{\frac{m(m-1)}{2}}\\
      \iff&\quad& \delta^{-2}  &\approx \alpha^{(m-1)/m}\\
    \end{align*}
    Hence, $alpha \approx \delta^{-2}$ and 
    \begin{equation}
      \| \tilde{\mathbf{b}}_i \| \approx \delta^{-2(i-1) + m} \det(\Lambda)^{1/m}
    \end{equation}

  * good basis -> first Gram-Schmidt vectors become shorter (latter longer)

  * $\delta = 1.01$ feasible, $\delta = 1.007$ seems infeasible for now

  * gap between provable and experimental cost estimate to reach some hermite $\delta$ => provable results only give upper bounds, for practical security we need lower bound => combine theoretical results with experimental results

  * well-established estimate \cite{LP11}

In the following, we will focus on two related methods for lattice reduction. % A third approach, the Hermite, Korkine, Zolotarev (HKZ) reduction %TODO: write a sentence or two about it?

% TODO: algorithm LLL reduction, check out CWX13
\subsection{The LLL Algorithm}
The LLL algorithm was proposed by Lenstra, Lenstra and Lovász \cite{LLL82} and can be considered as a generalization of the two dimensional Lagrange reduction. The lagrange reduction reduces a basis of two basis vectors such that output basis satisfies $\|\mathbf{b}_1\| \leq \|\mathbf{b}_2\|$ and $\frac{|\left\langle\mathbf{b}_1, \mathbf{b}_2\right\rangle|}{\|\mathbf{b}_1\|} = |\mu_{2,1}| \leq \frac{1}{2}$). Intuitively, a multiple of the shorter vector $\mathbf{b}_1$ is subtracted from the longer vector $\mathbf{b}_2$ such that the resulting vector $\mathbf{b}_2'$ is as orthogonal to $\mathbf{b}_0$ as possible, i.e.  $\mathbf{b}_1' =  \mathbf{b}_1 - \lfloor\mu_{1,0}\rceil \mathbf{b}_0$. We set $\mathbf{b}_2 = \mathbf{b}_2'$ and repeat until nothing changes. 

A $\delta$-LLL reduced basis ensures two criterias \cite{LLL82}:
\begin{enumerate}
  \item Size reduced: $|\mu_{i,j}| \leq \frac{1}{2}$ for $1\leq i \leq n$ and $j < i$ \label{size-red}
  \item Lovász condition: $\delta \| \tilde{\mathbf{b}}_i \|^2 > \| \mu_{i+1, i} \tilde{\mathbf{b}}_i + \tilde{\mathbf{b}}_{i+1} \|^2$ for $1\leq i < n$
\end{enumerate}

Recall the definition of the Gram-Schmidt coefficients $\mu_{i, j} = \frac{\left\langle \tilde{\mathbf{b}}_j, \mathbf{b}_i\right\rangle}{\left\langle \tilde{\mathbf{b}}_j, \tilde{\mathbf{b}}_j\right\rangle}$. The LLL algorithm shown in \cref{alg:LLL} follows the notation in \cite{LLLReg04}. We start by computing the Gram-Schmidt orthogonalization of the input basis (\cref{alg:LLL-start}) and continue with a reduction step in which we update every basis vector $\mathbf{b}_i$ by pairwisely comparing and subtracting lower indexed basis vector just as in the Lagrange reduction (\cref{alg:LLL-red}) to ensure Criteria \ref{size-red}. Finally, vectors violating the Lovász condition are swapped (\cref{alg:LLL-swap}ff) and the process is repeated until nothing changes. The LLL algorithm can be used to find short vectors of at most $2^{n/2} \lambda_1(\Lambda)$ in polynomial time. Several floating-point variants have been suggested that can significantly speed up the runtime of LLL. For example, L$^2$ runs in $\mathcal{O}(n^2 \log^2 B)$, where $B$ is a bound on the norm of the input basis vectors \cite{NS05}. % TODO: newer versions?

\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function} % FROM https://cims.nyu.edu/~regev/teaching/lattices_fall_2004/ln/lll.pdf
\Begin($\delta\text{-LLL} {(}\mathbf{B} \in \mathbb{Z}^{m\times n} {)}$) % TODO nxn???
{
  Compute $\tilde{\mathbf{B}}$\label{alg:LLL-start}\\
  \For{$i=2, \dots, n$}{ 
    \For{$j=i-1, \dots, 1$}{
      $\mathbf{b}_i = \mathbf{b}_i - \lfloor\mu_{i, j}\rceil \mathbf{b}_j$\label{alg:LLL-red}\\
    }
  }
  \If{$\exists i \text{ \upshape such that } \delta \| \tilde{\mathbf{b}}_i \|^2 > \| \mu_{i+1, i} \tilde{\mathbf{b}}_i + \tilde{\mathbf{b}}_{i+1} \|^2$}{\label{alg:LLL-swap}
    tmp = $\mathbf{b}_i$\\
    $\mathbf{b}_{i} = \mathbf{b}_{i+1}$\\
    $\mathbf{b}_{i+1} = \mathbf{b}_{i}$\\
    Return $\delta$-LLL($\mathbf{B}$)\\
  }
  \Else{
    Return $\mathbf{B}$\\
  }
}
\caption{The $\delta$-LLL Algorithm \cite{LLL82}} \label{alg:LLL}
\end{algorithm2e}



\subsection{The BKZ Algorithm} \label{sec:BKZ}
% Rewrite, from Pla18

The Block Korkin-Zolotarev (BKZ) algorithm was proposed by Schnorr in 1987 and adapted by Schnorr and Euchner in \cite{SE91} and represents a family of lattice reduction algorithm. Essentially, BKZ iteratively divides the input basis into blocks of a lower dimension $k$ and calling an SVP oracle on each block. The output of the oracle is then used to obtain a basis of improved quality. 

\cref{alg:BKZ} presents the main concept of BKZ and follows the description in \cite{CN11} with some adjustments. Initially, we run an LLL reduction on the input basis $\left\{\mathbf{b}_1, \dots, \mathbf{b}_{n}\right\}$ and update the basis. In each $j$th iteration, we consider a block of $k$ basis vectors $\mathbf{b}_j, \dots, \mathbf{b}_{j+k-1}$. The vectors of the current block are projected onto the orthogonal complement of the span of vectors from previous iterations $\text{span}\left(\left\{\mathbf{b}_i \mid i \in [j-1]\right\}\right)$ (\cref{BKZ-span} - \ref{BKZ-proj}, we skip this step if the span is empty). Note that the orthogonal complement $A^\perp$  of a subspace $A$ is defined as the set of all vectors that are orthogonal to every vector in $A$. We then run an SVP oracle on the projected block to obtain a shortest vector $\mathbf{b}_\text{new}'$ in the projected lattice (\cref{BKZ-svp}) and reconstruct a lattice vector $\mathbf{b}_\text{new}$ of which $\mathbf{b}_\text{new}'$ is a projection \cref{BKZ-rec}. Note that in practice, the SVP oracle should include this step. If $\mathbf{b}_\text{new}$ is a new vector we insert it in our list of basis vectors before $\mathbf{b}_j$. Otherwise as nothing changed, we increment a counter $z$. Finally, we run LLL on all basis vectors up to index $j+i$ (including the possibly newly added vector). If no new lattice vectors can be found in $n$ iterations, the reduction terminates. After $n$ iterations, $j$ is reset to start over at the first block. The ouput of the algorithm is a BKZ$_k$-reduced basis. For $k=2$ we obtain an LLL-reduced basis in polynomial time and for $k=n$ an optimally HKZ-reduced basis in at least exponential time.%TODO needed? probably not, maybe just mention k=2 case, Pla18


\begin{algorithm2e} % TODO: check if block is not of dimension kxk
  \SetKwBlock{Begin}{function}{end function}
  \Begin($\text{BKZ} {(} \mathbf{B} = \{\mathbf{b}_1, \dots, \mathbf{b}_{n}\}, k \in [n]\backslash\{1\} {)}$) % TODO nxn???
  {
    z = 1; j = 0\\
    $\mathbf{B} = \text{LLL}(\mathbf{B})$ \\% LLL reduce and update basis
    \While{$z < n-1$}{ 
      $j = (j \mod (n-1)) + 1; l = \min(j + k -1, n); h = \min(l + 1, n)$\\
      $A = \text{span}\left(\left\{\mathbf{b}_i | i \in [j-1]\right\}\right)$\label{BKZ-span}\\
      \For{$i \in \{j, ..., l\}$}{
        \If{$A \neq \emptyset$}{
          $\mathbf{b}_i' = \pi_{A^\perp}(\mathbf{\mathbf{b}_i})$\label{BKZ-proj}\\
        }
        \Else{
          $\mathbf{b}_i' = \mathbf{b}_i$\\
        }
      }     
      $\mathbf{b}_\text{new}' = \text{SVP-Oracle}(\mathbf{b}_j', \dots, \mathbf{b}_{l}')$\label{BKZ-svp}\\
      Reconstruct $\mathbf{b}_\text{new} = \textstyle \sum_{i=j}^l\alpha_i \mathbf{b}_i$ with $\alpha_i \in \mathbb{Z}$ such that $\mathbf{b}_\text{new}' = \pi_{\left(\text{span}\left(\mathbf{b}_j, \dots, \mathbf{b}_{l}\right)\right)^\perp}(\mathbf{b}_\text{new})$\label{BKZ-rec}\\
      \If{$\mathbf{b}_\text{new}' \neq \tilde{\mathbf{b}}_j$}{
        $z=0;$ $\left\{\mathbf{b}_j, \dots, \mathbf{b}_{h} \right\}= \text{LLL}(\left\{\mathbf{b}_j, \dots, \mathbf{b}_{j-1}, \mathbf{b}_\text{new}, \mathbf{b}_j, \dots, \mathbf{b}_{h} \right\})$\\
      }
      \Else{
        $z = z + 1;$ $\left\{\mathbf{b}_j, \dots, \mathbf{b}_{h} \right\} = \text{LLL}(\left\{\mathbf{b}_j, \dots, \mathbf{b}_{h} \right\})$\\
      }
    }
  }
  \caption{The BKZ Algorithm \cite{SE91}} \label{alg:BKZ}
\end{algorithm2e} % TODO: check, maybe also with ABLR21, Alg 2 or 4

% TODO: explain how SVP oracle can be implemented => possibly outsource, section for cost models!!!

Several improvements have been suggested. The total number of rounds until termination is unknown and can be quite large. Hanrot \textit{et al.} \cite{HPS11a} show an early termination of BKZ still yields a very good output basis quality and propose $\frac{n^2}{k^2} \log n$ rounds as a bound. Th
Local preprocessing increases the quality of the current block basis by recursively calling BKZ with smaller block size. A variant known as progressive BKZ applies the recursion globally \cite{AWHT16}. If enumeration is used as an SVP oracle (see \cref{sec:costmodels}), extreme pruning can be applied to significantly reduce the search space. Gamma \textit{et al.} show that applying such a bounding function on the search tree reduces the running time by a much larger factor than the success probability. Repeating the search yields the desired speedup \cite{GNR10}. In addition, \cite{CN11} optimizes the enumeration radius by using experimental results. BKZ 2.0 incorporates a number of these techniques \cite{CN11}.

% BKZ runtime bounds: TODO rewrite, from MW16
% - runtime (without early termination) grows superpolynomially in $n$ \cite{GN08b}
% - best provable bounds on ouput after termination worse than that of slide reduction by at least polynomial factor (slide reduction [\cite{GN08a}] yields best theoretical results but in practice worse than BKZ)
% - calls to SVP oracle in arbitrary dimensions up to block size => runtime depends on worst-case Hermite constants of each block size 
% => simulation of BKZ needs to include estimation of runtime of SVP oracle for each block size
% TODO: use the following? 
% Versions:
% - slide reduction yields best theoretical result (2^n log log n /log n) \cite{GN08a} 
% - practical floating point version of LLL \cite{SE91} best practical results 

It is difficult to find hard runtime bounds for BKZ. The upper bound on the number of rounds is superexponential in the dimension $n$ for a fixed block size \cite{HPS11a, GN08b} before BKZ terminates by itself. In addition, calls to the SVP oracle is called in all dimensions $k' \leq k$ must be taken into account. 
Albrecht \textit{et al.} ignore these intricacies and estimate the cost of BKZ in clock cycles as $\rho \cdot n \cdot t_k$ where $\rho$ is the number of rounds needed and $t_k$ is the cost (in block cycles) of calling the SVP oracle on a block of dimension $k$ \citet{APS15}. The value $\rho$ is set to $8$ in the \textit{Estimator} derived from experiments in \cite{Chen13} that indicate that the most significant progress happens in the first $7-9$ rounds. 

% TODO: include? \cite{LP11} runtime according Linder-Peikert model, GSA: suggest $t_{\text{BKZ}}=1.8/\log_2{\delta} - 110$


% TODO perhaps look into: General Sieve Kernel implementation \cite{ADHKPS19} beats BKZ + pruned enum 



\subsection{Cost Models for Lattice Reduction} \label{sec:costmodels}
% TODO. write little introduction
In this section, we will look at various high level ideas to realize an SVP solver that can be used as a subroutine in BKZ and present up-to-date cost models from the literature. SVP is known to be NP-complete even for large constant approximation factors \cite{Ajt98, Khot05}. An exponential approximation factor can be achieved in polynomial time but is mostly insufficient for practical purposes \cite{LLL82}.  We will mainly focus on two classes of (nearly) exact SVP solvers, namely, enumeration algorithms and sieving algorithms. Enumeration algorithms can solve SVP in a lattice of dimension $k$ in $2^{\mathcal{O}(k \log k)}$ time and polynomial space. Sieving algorithms only need $2^{\mathcal{O}(k)}$ time, however, at the cost of exponential memory complexity. Only recently, progress in sieving strategies has given rise to BKZ implementations relying on sieving (e.g. the General Sieve Kernel (G6K) impelmentation \cite{ADHKPS19, DSW21}) that outperform enumeration implementations already in relatively small dimensions $\gtrsim 70$ in the classical setting \cite{ABLR21}. On the other hand, quantum speedups for enumeration are greater than for sieving. \citet{ANS18} show a quadratic cost reduction for enumeration, while the cost sieving only decreases by a factor of $2^0.027$ with idealized assumptions \cite{Laa15}. % assuming no depth restriction and quantum accessible RAM (qRAM)


% nice overview in https://cseweb.ucsd.edu/~daniele/LatticeLinks/Enum.html
\subsubsection{Enumeration} % see Chen13 for a better explanation
Enumeration aims to find the shortest vector by enumerating all lattice vectors within some bounded region. In general, we start with reducing the lattice basis to improve the basis quality. We then define a bound and iteratively project the lattice to the span of its Gram-Schmidt vectors beginning from $\tilde{\mathbf{b}}_n$ until we arrive at the lowest level of a one-dimensional subspace. We continue by enumerating all vectors of norm less than $r$ in the projected lattice and ``lift'' each of these vectors to the level above and repeat this process until we arive at the level from which we started. The search space can be thought of as a large tree of (projected) vectors on which we apply depth-first search. Note that the root of the tree here is at the lowest level and the leafs are the lattice vectors in our target lattice. %Consider a two-dimensional lattice with basis $\left\{\mathbf{b}_1, \mathbf{b}_2 \right\}$ and bound$r = \| \mathbf{b}_1 \|$. We project the lattice to the span of the Gram-Schmidt vector $\tilde{\mathbf{b}}_2$ and enumerate all the vectors within the  $r$... Not really needed
The low memory cost of enumeration is due to its similarities to depth-first search. 

A very early but very efficient variant was suggested by \citet{Kan83} with a worst-case runtime of $2^{\mathcal{O}(k \log k)}$. BKZ$_k$ using Kannan's enumeration algorithm as SVP oracle yields a short lattice vector of norm $\approx (k^{1/(2k)})^n\cdot \text{Vol}(\Lambda)^{1/n}$ \cite{HS07, ABFKSW20}. % TODO check if n or m
% TODO: write sth about root hermite factor, ... for the next sections or change next sections

In \cite{Chen13, ABFKSW20} we find a more concrete cost model of $\text{poly}(n) \cdot 2^{1/(2e) k \log k + - 0.995 k + 16.25}$ for BKZ 2.0 (see \cref{sec:BKZ}), where $\text{poly}(n)$ is the number of calls to the enumeration subroutine. BKZ 2.0 achieves a  root Hermite factor of $\left(\frac{k}{2\pi e} \cdot (\pi k)^{1/k}\right)^{\frac{1}{2(k-1)}}$. 

% TODO add to BKZ variants!!! => extended preprocessing and relaxing of search radius, 
% also add G6K (General Sieve Kernel) from ADHKPS19 (see more in sieving), running time of 2^Theta(k) but also 2^Theta(k) memory
The FastEnum algorithm in \citet{ABFKSW20} incorporates an idea called ``extended preprocessing'' and achieves a root Hermite factor of $k^{\frac{1}{2k}(1 + o(1))}$ in $\text{poly}(n) \cdot 2^{0.125k \log k - 0.050k + 56}$ time. The corresponding quantum algorithm reduces the runtime to $k^{k/16 + o(k)}$. In extended preprocessing, instead of preprocessing the current projected basis block of size $k$, the BKZ-reduction is applied to a block of higher dimension $\lceil (1+c)\cdot k\rceil$ for some constant $c$. Enumeration is faster on the first basis vectors as their Gram-Schmidt norms closely follow the Geometric Series Assumption \cite{MW16}. % TODO: move to BKZ ??? And perhaps switch BKZ with this section?

A tradeoff of runtime and success probability for ``relaxing'' the approximation and extreme pruning turns out to exponentially speed up the search \cite{LN20} and was combined with extended preprocessing in \cite{ABFKSW20} to further reduce the runtime of BKZ to $\text{poly}(n) \cdot 2^{0.125k \log k - 0.654k + 25.84}$ for a root Hermite factor of $k^(1/(2k))$. 

% TODO: what to do with poly(n)? just throw it away? 


% TODO: for table
% BKZ 2.0 & \cite{CN11, Chen13, ABFKSW20} & $2^{0.184k \log k - 0.995k + 16.25}$
% ABF+ Enum & \cite{ABFKSW20} & $2^{0.125k \log k}$ => root Hermite factor of $k^(1/(2k))$
% ABF+ Enum + O(1)& \cite{ABFKSW20} & $2^{0.125k \log k - 0.547k + 10.4}$
% ABF Q-Enum & \cite{ABFKSW20} & $2^{0.0625 k \log k}$ 
% ABLR Enum + O(1)& \cite{ABLR21} & $2^{0.125k \log k - 0.654k + 25.84}$



\subsubsection{Sieving} 
The second group of SVP solvers are sieving algorithms. In sieving, initially, we create a long list of randomly selected lattice points. The points in the list are then combined or ``reduced'' in some way to find points of smaller length. One way to achieve this is by finding a minimimal sublist of ``center'' points in the initial list such that spheres centered at these points cover all points list points. Subtracting the center points yields short lattice points. ListSieve \cite{MV10} uses a smaller initial list to divide the space into two half-spaces, one closer to the center and one closer to the respective point. The list is then used to reduce the length of newly sampled points as much as possible by subtracting each list vectors such that the result is located in the half-space closer to the center respectively. Once two points with a distance less than the target distance are found, they are subtracted and the result is returned. 
% TODO: nearest neighbor speedups: combining two vectors can only result in reduction only if their pairwise angle is less than $60°$. A candidate check of two vectors of similar length can thus be carried out by testing if the angle is at most $\pi/3$. \cite{BDGL16}

% nice overview and explanation in \cite{ADHKPS19}
\begin{table}[h]
  \centering
  \begin{tabular}{ll}
    \toprule
    Name & Cost Estimate \\\hline
    List Sieve \cite{MV10} & $2^{0.3199n + o(n)}$ time, $2^{0.1325 + o(n)}$ memory\\
    NV-sieve \cite{NV08, ADHKPS19} & $2^{0.415n + o(n)}$ time, $2^{0.2075n + o(n)}$ memory\\
    NV-sieve (quantum) \cite{NV08, ADHKPS19} & $2^{0.311n + o(n)}$ time, $2^{0.2075n + o(n)}$ memory\\
    Gauss sieve \cite{MV10, HK17} & $2^{0.415n + o(n)}$ time, $2^{0.2075n + o(n)}$ memory\\
    BGJ-sieve \cite{BGJ15} & $2^{0.311n + o(n)}$ time, $2^{0.2075n + o(n)}$ memory\\
    3-sieve \cite{BLS16, HK17} & $2^{0.3962n + o(n)}$ time, $2^{0.1887n + o(n)}$ memory\\
    BDGL-sieve \cite{BDGL16} & $2^{0.292n + o(n)}$ time\\
    BDGL-sieve (quantum) \cite{BDGL16} & $2^{0.265n + o(n)}$ time \\
    \bottomrule
  \end{tabular}
  \caption{Sieving Algorithms} % TODO: cite? from estimate all the?
  \label{tab:sieving}
\end{table}

\cref{tab:sieving} presents a list of currently best sieving algorithms. 
In the Nguyen-Vidick sieve \cite{NV08}, we iteratively reduce a pair of list points whose combined length is smaller than the longest list vector. The longest vector is then replaced by the result. The list length is fixed. 
In the Gauss sieve \cite{MV10}, we start with an empty list and a stack. In each step, a new point is either sampled or taken from the stack. We then attempt to reduce the new point with all points in the list. If a reduction is successful, the longer vector of the pair is replaced. If the longer vector was the list point, the replacement is inserted in the stack. If no reduction is possible, the stack points are moved back to the list. If the stack is empty, all list points are reduced pairwisely. In practice, the Gauss sieve outperforms Nguyen-Vidick sieve. 
The Becker-Gama-Joux sieve \cite{BGJ15} exploits coding theory to find vectors that are likely to be nearest neighbors. Similar vectors are stored in the same bucket to speed up the search for reduction candidates.
The 3-sieve \cite{BLS16, HK17} reduces the required list size by using triples instead of pairs of points for combination.
Finally, the Becker-Ducas-Gama-Laarhoven sieve \cite{BDGL16} applies locality sensitive hashing to create buckets of points in near neighborhood similar to the Becker-Gama-Joux sieve. % CHECK if that is ListDecoding

TODO: small section about use in tool... \cref{tab:costmodels} provides an overview of the cost models that are built into the Lattice Parameter Estimation.






% TODO: check out AKS01 algorithm 

% recent progress: % from DSW21
% - theoretical: Laa15, BGJ15, BDGL16, HKL1
%   techniques: 
%     Nearest Neighbor Search (Laa15)
%     Progressive Sieving (LM18)
%     Dimensions for Free (Duc18): lift many short vectors in sieving dimension to recover short vector in larger dimensions
% - practical performance: FBB+14, Duc18, LM18, ADHKPS19

% quantum: sieving uses near neighbor search as subroutine, which in turn uses black box search, Grover's quantum search algorithm reduces the complexity of black box search by square root \cite{Gro97}


% TODO also check out LM18, Duc18, ADHKPS19



%of length smaller than some bound $r$. Then, we choose some center points $\mathbf{v}'_i, i \in [l]$ for some $l \ll 2^{cn}$ of the list such that all points $\mathbf{v}_i$ of the initial list are covered by spheres of a radius $r' < r$ centered at these center points $\mathbf{v}'_i$. Finally, we compute short vectors by subtracting the centers. % TODO


% - CN11, APS15, ADPS16
% sieving \cite{BDGL16, Laa15,ADPS16, APS15,BDGL16}
% enumeration \cite{Kan87, MW15,FP85, CN11, APS15, HPSSWZ17}
% cost models (tabular overview) \cite{ACCD+19}
% - number of SVP calls \cite{ADPS16,Alb17}
% - upper bound on rounds is exponential \cite{HPS11a, GN08b}
% - lattice rule of thumbs: achievable root hermite factor $\delta_0 = k^{\frac{1}{2k}} $ % [Stehlé12 An introduction to lattice reduction] or 2^1/k [Stehlé13/16  An overview of lattice reduction algorithms] (oversimplification)





% primarily studied for SVP in euclidean norm
% two main types: 
% Classic Sieve \cite{AKS01}: create a long list of lattice vectors, then find shorter lattice vectors and discard longer
% List Sieve \cite{MV10}: start from empty list, find shorter vectors and append them to list

% best provable: $2^{n + o(n)}$ (sieving by averages)
% heuristic state of the art: $(3/2)^{n/2 + o(n)} \approx 2^{0.29n}$ 

% combinarial: create list of short random (possibly non-lattice) vectors, try to combine vectors in list to produce short lattice vectors
% provable versions not very useful (exponential space, random perturbations) but basis of heuristic variants used in practice 


% TODO: decide whether to take others
% \subsubsection{Others}
% - discrete Gaussian sampling % \cite{ADRS15}, adps?
% % TODO insert a table and reference somewhere else? Warum notwendig, wie kommt man darauf? ... alg laufen lassen, extrapolieren...
% - Micciancio-Voulgaris Algorithm - Voronoi-cell algorithm \cite{MV13} in $4^{n + o(n)}$ time and $2^{n+o(n)}$ space


% TODO: make chronology? 
% \begin{figure}
%   \begin{chronology}[5]{2000}{2021}{3ex}[\textwidth]
%     \event{2001}{AKS sieve $2^{\mathcal{O(n)}}$ time and space \cite{AKS01}}
%     \event{2010}{List sieve in $2^{3.2n}$ time and $2^{1.33n}$ space \cite{MV10}}
%     \event{2013}{Voronoi cell $2^{2n}$ time and $2^{n}$ space \cite{MV13}}
%     \event{2014}{Gaussian sampling $2^{n}$ time and space \cite{ADRS14}}
%   \end{chronology}
%   \caption{Provable Algorithms for SVP}
% \end{figure}

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    Name & Reference & Cost model \\\hline
    Q-Core-Sieve & \cite{Laa15,ADPS16,AGPS20} & $2^{0.265n}$\\
    Q-Core-Sieve + O(1) & \cite{SAL+17} & $2^{0.265n + 16}$\\
    Q-Core-Sieve (min space) & \cite{SHRS17} & $2^{0.2975n}$\\
    Q-$\beta$-Sieve & \cite{NAB+17} & $n \cdot 2^{0.265n}$\\
    Q-$8d$-Sieve & \cite{BAA+17} & $8d \cdot 2^{0.265n + 16.4}$\\
    Core-Sieve & \cite{BDGL16,ADPS16,AGPS20} & $2^{0.292n}$\\
    Core-Sieve + O(1) & \cite{SAL+17} & $2^{0.292n + 16}$\\
    Core-Sieve (min space) & \cite{SHRS17} & $2^{0.368n}$\\
    $\beta$-Sieve & \cite{NAB+17} & $n \cdot 2^{0.292n}$\\
    $8d$-Sieve & \cite{DTGW17} & $8d \cdot 2^{0.292n + 16.4}$\\
    Q-Core-Enum + O(1)& \cite{SHRS17, Chen13,ACDDPPVW18} & $2^{\frac{0.18728n \log n - 1.0192 n + 16.1}{2}}$\\
    Lotus (Enum)& \cite{PHAM17, ACDDPPVW18} & $2^{0.125n \log n - 0.755 n + 2.254}$\\
    Core-Enum + O(1)& \cite{SHRS17, Chen13,ACDDPPVW18} & $2^{0.18728n \log n - 1.0192 n + 16.1}$\\
    $8d$-Enum (quadratic fit) + O(1) & \cite{BC0V17} & $8d \cdot 2^{0.000784 n^2 + 0.366 n + 0.875}$\\
    BKZ 2.0 Core-Enum & \cite{CN11, Chen13, ABFKSW20} & $2^{0.184k \log k - 0.995k + 16.25}$\\
    ABF+ Core-Enum & \cite{ABFKSW20} & $2^{0.125k \log k}$\\
    ABF+ Core-Enum + O(1)& \cite{ABFKSW20} & $2^{0.125k \log k - 0.547k + 10.4}$\\
    ABF Q-Core-Enum & \cite{ABFKSW20} & $2^{0.0625 k \log k}$ \\
    ABLR Core-Enum + O(1)& \cite{ABLR21} & $2^{0.125k \log k - 0.654k + 25.84}$ \\
    \bottomrule
  \end{tabular}
  \caption{Cost Models Overview} % TODO: cite? from estimate all the?
  \label{tab:costmodels}
\end{table} % TODO: add to tool, maybe change 8d thing to an optional parameter!!! => reduces number of lists, throw out silly ones...
% TODO reference in text
% TODO: add description of "core"


\section{Algorithms for Solving LWE}
\subsection{Overview}
% TODO from LP11, change
% TODO: maybe include graphic of three approaches
Distinguishing attacks (MR09, RS10): distinguish (with noticeable advantage) LWE instance from uniformly random => break semantic security of LWE-based cryptosystem with same advantage (typically), find short nonzero integral vector $\mathbf{v}$ s.t. $\mathbf{A}^\intercal \mathbf{v} = 0 \mod q$ => short vector in (scaled) dual of LWE lattice $\Lambda(\mathbf{A})$ % for A mxn matrix else switch t
then test whether $\langle \mathbf{v}, z \rangle$ is close to zero mod q. If uniform test accepts with prob 1/2, if LWE with parameter s, $\langle \mathbf{v}, \mathbf{z} \rangle = \langle \mathbf{v}, \mathbf{e} \rangle \mod q$, Gaussian mod q with parameter $\| \mathbf{v} \| \cdot s$. If that's not much larger than q, advantage for distinguishing very close to $\exp(-\pi (\| \mathbf{v} \| s/q)^2)$. high confidence needs $\| \mathbf{v} \| \leq q/(2s)$ 
advantage an computational effort need to be balanced (often inverse distinguising advantage is in total cost of attack)

\subsubsection{Dual Attacks}
reduce LWE to SIS
recover secret vector by finding a short vector in the dual lattice $\Lambda(\mathbf{A}^\intercal)^{\perp} = \{ \mathbf{y} \in \mathbb{Z}^m \mid \mathbf{A} \mathbf{y} = \mathbf{0} \mod q\}$ generated by the rows of $\mathbf{A}$ and scaled by q.
\subsubsection{Primal Attacks}

lattice reduction algorithms solve SIS and BDD 


\subsubsection{Direct} % or combinatorial

% TODO: based on GSJ15, rephrase
- algebraic approach Arora and Ge with subexponential complexity when $\sigma \leq \sqrt{n}$, else fully exponential, mainly of asymptotic interest (higher complexity than others)


- combinatorial algorithms: BKW as basis \cite{BKW03}, resembles generalized birthday approach by Wagner, % Wagner, D.: A generalized birthday problem. In: Advances in cryptology CRYPTO 2002, pp. 288304. Springer (2002)
originally for solving LPN, can be analyzed => explicit complexity for different LWE instances, theoretical analysis and actual performance close,
very memory expensive (often same order as time complexity) 



\subsection{BKW \cite{BKW03}}
The Blum, Kalai and Wasserman (BKW) algorithm was originially designed to solve the Learning Parity with Noise problem (LPN) \cite{BKW03}. In \cref{sec:lwe} we pointed out that LPN is a subproblem of LWE and Albrecht \textit{et al.} adapted BKW to LWE in \cite{ACFFP15a}. The runtime and memory complexity of BKW is in $2^{\mathcal{O}(n)}$ for an LWE instance with secret dimension $n$ prime modulus $q \in \text{poly}(n)$. The number of samples $m$ must be sufficiently large (in $\mathcal{O}(n \log n)$). % TODO check number of samples, APS15 say they need unrestricted access to LWE oracle
% TODO: 

BKW falls into the regime of dual attacks, that is, it solves LWE by finding a short vector $\mathbf{s}$ in the scaled dual lattice $\Lambda(\mathbf{A}^\intercal)^{\perp}$. 

% TODO: not sure if lattice vectors are only in Z_q^m (according to APS15), TODO: how exactly is BKW in the dual lattice? intuition missing... if it uses SIS strategy, maybe consider using it for solving SIS? problem: number of samples?

three stages \cite{ACFFP15a}: sample reduction, hypothesis testing and back substitution

\paragraph{Sample Reduction.} In the following, we present an outline of the main BKW algorithm. The steps in \cref{alg:BKW} are inspired by the textual description in \cite{GJS15} with minor adjustments in notation. 

% TODO: preprocessing: need secret distribution that follows error distribution? \cref{sec:lwe-decoding}


\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function}
\Begin($\text{BKW} {(}\mathbf{A} \in \mathbb{Z}^{n\times m},\mathbf{z} \in \mathbb{Z}^m, b \in \mathbb{Z}, d\in \mathbb{Z}{)}$)
{
  $i = 1$\\
  $\mathbf{A}^{(i)} = \mathbf{A}$\\
  $\mathbf{z}^{(i)} = \mathbf{z}$\\
  \While{the last $n-d$ coefficients of the columns of $\mathbf{A}^{(i)}$ are nonzero}{ % TODO
    // BKW step\\
    $j = 1$\\
    $\mathbf{T}^{(i)} = []$ \Comment Collision table\\
    \For{$k = 1, \ldots, m^{(i)}$}{
      // $m^{(i)}$ is number of columns in $\mathbf{A}^{(i)}$\\
      \uIf{last $(i\cdot b)$ coefficients of $\mathbf{a}_k^{(i)}$ are zero}{
        $\mathbf{a}_j^{(i+1)} = \mathbf{a}_k^{(i)}$\\
        $z_j^{(i+1)} = z_k$\\
        $j = j + 1$\\
      }
      \uElseIf{no match for $\mathbf{a}_k^{(i)}$ in $\mathbf{T}$}{
        $\mathbf{T} = \mathbf{T} + \left[\mathbf{a}_k^{(i)}\right]$ \Comment append to collision set
      }
      \uElseIf{match $\mathbf{a}_l^{(i)}$ for $\mathbf{a}_k^{(i)}$ is found}{
        \uIf{$\mathbf{a}_l^{(i)}$ matches $\mathbf{a}_k^{(i)}$ in the last $(i\cdot b)$ components}{
            $\mathbf{a}_j^{(i+1)} = \mathbf{a}_k^{(i)} - \mathbf{a}_l^{(i)}$; \Comment last $i \cdot b$ coefficients of $\mathbf{a}_j^{(i+1)}$ are now zero\\
            $z_j^{(i+1)} = z_k^{(i)} - z_l^{(i)} = y_j^{(i)} + e_j^{(i)}$, where $y_j^{(i)} = \left\langle \mathbf{s}, \mathbf{a}_j^{(i)}\right\rangle$ and $e_j^{(i)} = e_k^{(i)} - e_l^{(i)}$ \label{alg:BKW-z1}\\ 
            $j = j + 1$\\
        }
        \uElseIf{the negation of $\mathbf{a}_l^{(i)}$ in $\mathbb{Z}_q^n$ matches $\mathbf{a}_k^{(i)}$ in the last $(i\cdot b)$ components}{
          $\mathbf{a}_j^{(i+1)} = \mathbf{a}_k^{(i)} + \mathbf{a}_l^{(i)}$\\
          $z_j^{(i+1)} = z_k^{(i)} + z_l^{(i)} = y_j^{(i)} + e_j^{(i)}$, where $y_j^{(i)} = \left\langle \mathbf{s}, \mathbf{a}_j^{(i)}\right\rangle$ and $e_j^{(i)} = e_k^{(i)} + e_l^{(i)}$\label{alg:BKW-z2}\\
          $j = j + 1$\\
        } % TODO: maybe put both cases into one step?
      }
    }
    $i = i + 1$\\
    // Calculate input for next BKW step\\
    $\mathbf{A}^{(i)} = (\mathbf{a}_1^{(i)} \cdots \mathbf{a}_{j-1}^{(i)})$\\
    $\mathbf{z}^{(i)} = (z_1^{(i)}, \ldots, z_{j-1}^{(i)})$\\
  }
  Return $(\mathbf{A}^{(i)}, \mathbf{z}^{(i)})$
}
\caption{BKW (Sample Reduction)}\label{alg:BKW}
\end{algorithm2e} % TODO check

For the algorithm, we use the matrix notation of LWE as in \cref{eq:lwe-decoding}, i.e. $\mathbf{z} = \mathbf{A}^\intercal \mathbf{s} + \mathbf{e}$. BKW consists of a series of BKW steps that iteratively reduce the dimension of input matrix $\mathbf{A}$ by finding collisions of its column vectors in the currently examined block of $b$ entries. We start from the last $b$ entries of $\mathbf{A}^{(1)} = \mathbf{A}$. In every step $i$, we maintain a collision table $\mathbf{T}^{(i)}$ and loop over the columns $\mathbf{a}_k^{(i)}$ of $\mathbf{A}^{(i)}$ and distinguish between the following cases: (1) If $\mathbf{a}_k^{(i)}$ only has zero entries in the examined block, pass $\mathbf{a}_k^{(i)}$ and $z_k^{(i)}$ to the next step, (2) if no match of $\mathbf{a}_k^{(i)}$ or the negation of $\mathbf{a}_k^{(i)}$ can be found in the collision table, add $\mathbf{a}_k^{(i)}$ to the colission table, and (3) if a match $\mathbf{a}_l^{(i)}$ is found, compute $\mathbf{a}_l^{(i)} + \mathbf{a}_k^{(i)}$ or in the case of a negation match $\mathbf{a}_l^{(i)} - \mathbf{a}_k^{(i)}$ (in $\mathbb{Z}_q$) such that the last $b$ nonzero entries cancel out. By exploiting the symmetry of $\mathbb{Z}_q$ in this way, in every step we obtain at most $(q^b - 1)/2$ columns with distinct coefficients in the current $b$ entries. We also make note of ``observed symbols'' $z_j^{(i)}$ that represent the combination of two samples given their respective matching columns (see lines \ref{alg:BKW-z1}, \ref{alg:BKW-z2} for more details). 

In each BKW step, the number of columns (and samples) decreases by at least $(q^b - 1)/2$ (size of the colusion set) and the variance of the error distribution $\sigma^2$ increases by a factor of two. The algorithm terminates after $t = \lceil b / (n - d)\rceil$ steps returns a a set of observed symbols $\mathbf{z}^{(t)}$ and a corresponding reduced matrix $\mathbf{A}^{(t)}$ in which only the first $d$ rows have nonzero entries. Parameter $d$ should be set to $1$, as in the original BKW algorithm, or $2$ for the best performance \cite{ACFFP15a}.

The remaining part $\mathbf{s}'$ of the secret vector $\mathbf{s}$ is then guessed by means of hypothesis testing. After $t$ steps the error term $\left(\mathbf{z}_j^{(t)} - \left\langle \mathbf{s}', \mathbf{a}_j^{(t)}\right\rangle\right)$ with $j \in [m']$ of the $m'$ remaining observed symbols follows a Gaussian distribution $\chi$ with noise $\sigma'^2 = 2^t\cdot \sigma^2$ (see Lemma 1 in \cite{ACFFP15a}). % TODO: Lemma: Let $X_0, \dots, X_{m-1}$ be independent random variables with $X_i \tilde \mathcal{N}(\mu, \sigma^2)$. Then their sum $X = \sum_{i=0}^{m-1} X_i$ is distributed according to $\mathcal{N}(m\mu, m\sigma^2)$.
We can test the noise of the error term for all $\mathbf{s}'' \in \mathbb{Z}_q^d$ against the hypothesized noise  $\sigma'^2$ by means of the log-likelihood ratio (for details we again refer to \cite{ACFFP15a}) and are thus able to determine $\mathbf{s}'$ given sufficiently many samples $m'$. 

Finally, we can apply back substitution to recover all elements of $\mathbf{s}$. We again apply a similar procedure as in \cref{alg:BKW} to reduce a number of columns from the colission tables computed in the Sample Reduction step and obtain $m'$ columns with $d+d'$ nonzero entries and their corresponding ``observed symbols''. We then substitute the part of $s$ that was recovered in the previous steps and recover the next part of $\mathbf{s}$ by hypothesis testing and repeat the process until we have found $\mathbf{s}$. 

% TODO: runtime complexity change to APS15 Theorem 3, 4 (DTV15) or Cor 4, or try to take complexity from GSJ15
\begin{theorem}[BKZ Complexity \cite{ACFFP15a}, Corollary 2]
  Let $(\mathbf{a}_i, z_i)$ be samples following $\mathcal{A}_{\mathbf{s}, \chi}$, set $a = \lfloor \log_2(1/(2\alpha)^2)\rceil$, $b = n/a$ and $q$ a prime. Let $d$ be a small constant $0 < d < \log_2(n)$. Assume $\alpha$ is such that $q^b = q^{n/a} = q^{n/\lfloor \log_2(1/(2\alpha)^2)\rceil}$ is superpolynomial in $n$. Then, given these parameters the cost of the BKW algorithm to solve Search-LWE is 
  \begin{equation}\label{eq:BKZ-complexity}
    \left(\frac{q^b-1}{2}\right) \cdot \left(\frac{a(a-1)}{2} \cdot (n + 1) \right) + \left\lceil(\frac{q^b}{2}\right\rceil \cdot \left(\left\lceil(\frac{n}{d}\right\rceil + 1\right) \cdot d \cdot a + \text{poly}(n) \approx (a^2 n) \cdot \frac{q^b}{2}
  \end{equation}
  operations in $\mathbb{Z}_q$. Furthermore,
  \begin{equation}
    a \cdot \left\lceil(\frac{q^b}{2}\right\rceil + \text{poly}(n)\textit{ calls to } \mathcal{A}_{\mathbf{s}, \chi}\textit{ and storage of }  \left(a \cdot \left\lceil(\frac{q^b}{2}\right\rceil \cdot n\right) \textit{ elements in } \mathbb{Z}_q \textit{ are needed.}
  \end{equation}
\end{theorem}

The first summand of \cref{eq:BKZ-complexity} roughly corresponds to the cost of creating the colission tables and the second summand is the cost of backsubstitution. For a more detailed cost analysis, see Theorem 2 in \cite{ACFFP15a}.


\subsubsection*{Coded-BKW \cite{GJS15}}
- change BKW step -> more column entries are removed, but additional noise
- index set $I$, $\mathbf{x}_I$ is part of $\mathbf{x}$ with entries indexed by $I$
- step $i$: $I$ set of $b$ positions to be removed, fix some $q$-ary linear $\left[N_i, b\right]$ code $\mathcal{C}_i$ with $q^b$ codewords, find the closest codeword $\mathbf{c}_I \in \mathcal{C}$ for every input vector $\mathbf{a}_I$ such that $\mathbf{a}_I = \mathbf{c}_I + \mathbf{e}_I$, where the error part $\mathbf{e}_I \in \mathbb{Z}_q^{N_i}$ is minimized by a decoding procedure.

Finally, we subtract two vectors and their correpsonding samples and pass the result to the next BKW step. Consider the inner product$\left\langle \mathbf{s}_{I}, \mathbf{a}_{I} \right\rangle = \left\langle \mathbf{s}_{I}, \mathbf{c}_{I} \right\rangle + \left\langle \mathbf{s}_{I}, \mathbf{e}_{I} \right\rangle$. In the subtraction, only the error part $\left\langle \mathbf{s}_{I}, \mathbf{e}_{I} \right\rangle$ remains. 

% TODO
% TODO: cost estimate??? too complex to include here...




% - Various improvements have been suggested since. For example, \cite{AFFP14} and \cite{KF15} use modulus switching for binary-LWE and other small secret variants \cite{AFFP14} and multidimensional Furier transformations can be used to speed up operations \cite{DTV15}. % TODO define binary LWE \cite{AFFP14} and find others, what is modulus switching, how do they apply FFT?
% lazy modulus switching \cite{KF15}: modulus is switched from $q$ to some $p<q$ and each coefficient $x$ of vectors to be compared is set to $xp/q \in $\mathbf{Z}_p$. We obtain an additional error term, but the number of vectors needed in a block is reduced from $q^b$ to $p^b$. The modulus switching can be applied in each step with decreasing $p$ as the error term due to the standard BKW cancel operations increases and thus allows for a larger additional error term.

% modified BKW step -> coded-BKW step to cancel out more positions in the $\mathbf{a}$ vectors than traditional BKW step

% map part of $\mathbf{a}$ vector into nearest codeword in lattice code (linear code over $\mathbb{Z}_q$, Euclidean distance)

% introduces some noise, can be kept small by appropriate parameters

% pair of $\mathbf{a}$ vectors map to same codeword => add together to create new sample with part of $\mathbf{a}$ vector cancelled

% samples are input to next step in BKW procedure

% additional steps using discrete FFT

% slightly modified for BINARY-LWE (secret vector uniformly chosen from $\{0, 1\}^n$) greatly increases performance




\subsection{Dual Attack \cite{MR09}}
"Gama and Nguyen \cite{GN08b}: (in)feasibility of obtaining various Hermite factors
natural distinguishing attack on LWE by finding one relatively short vector in associated lattice"
% TODO: check out 6.4 in ADPS16
% TODO: main description in SIS section




\subsection{Decoding Attack \cite{LP11}} \label{sec:decoding}

combines lattice basis reduction followed by an enumeration algorithm (bounded-distance decoding with preprocessing?) => time/success tradeoff
specifically for LWE, exploits structural properties of LWE
on search version of LWE problem, approach preferable to distinguishing attack on decision LWE in \cite{MR09, RS10}, same or better advantage than distinguishing attack using lattice vectors of lower quality => runtime is smaller
post-reduction: simple extension of Babai's ``nearest-plane'' algorithm \cite{Bab85} % TODO describe
=> trade basis quality against decoding time
related to Klein's (de)randomized algorithm \cite{Kle00} for bounded-distance decoding

use entire reduced basis, post-reduction part is fully parallelizable
% prerequisites: babai's nearest plane algorithm (at least describe textually?), fundamental parallelipiped P_1/2


% TODO: algorithm Babai's NearestPlane % FROM https://cims.nyu.edu/~regev/teaching/lattices_fall_2004/ln/lll.pdf
LLL reduction to input Lattice, integer combination of basis vectors close to target (like inner loop in reduction step of LLL), seek vector in lattice close to target, finds output that is in fundamental parallelipiped $\mathcal{P}(\mathbf{B})$ \cref{eq:fundamental-parallelipiped} => if error vector not in $\mathcal{P}(\mathbf{B})$, secret is not restored % TODO
=> basis quality has to be sufficiently good
\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function}
\Begin($\text{NearestPlane} {(}\mathbf{B} \in \mathbb{R}^{m \times n},\mathbf{t}\in \mathbb{R}^{m}{)}$)
{
  run $\delta$-LLL on basis $\mathbf{B}$ with $\delta=\frac{3}{4}$\\ % TODO put outside of algorithm
  $\mathbf{b} = \mathbf{t}$\\
  \For{$i = n, \dots, 1$}{
    $c_i = \text{round}(\langle \mathbf{b}, \tilde{\mathbf{b}}_i\rangle /  \langle \tilde{\mathbf{b}}_i, \tilde{\mathbf{b}}_i\rangle)$
    $\mathbf{b} = \mathbf{b} - c_i \mathbf{b}_i$ 
  }
  output $\mathbf{t} - \mathbf{b}$
}
\caption{Babai's Nearest Plane Algorithm \cite{Bab85}}\label{alg:babai} % TODO change algorithm to match below or just take intuitive description
\end{algorithm2e}

Output is a lattice vector $\mathbf{v} \in \Lambda(\mathbf{B})$ such that $\|\mathbf{v} - \mathbf{t}\| \leq 2^{n/2} \text{dist}(\mathbf{t}, \Lambda(\mathbf{B}))$ % define distance to lattice, change 

Goal: recover lattice vector relatively close to target vector
Intuition:
 - project $\mathbf{t}$ to $\text{span}(\mathbf{B})$
 - from $i=n, \dots, 1$ find closest hyperplane $c_i \tilde{\mathbf{b}}_i + \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_i)$ to the projection, subtract $c_i \mathbf{b}_i$ from the projection and continue % a little more detail
 - output vector is $\sum_{i=1}^n c_i \mathbf{b}_i$
% TODO: better description \as in BBDFGM14
for every basis vector $\mathbf{b}_i$ find $c_i$ such that distance between target and hyperplane spanned by $\mathbf{b}_1, ..., \mathbf{b}_{i-1}$ and shifted by $c_i \tilde{\mathbf{b}}_i$  is minimal % by using Gram-Schmidt vector
, subtract $c_i \mathbf{b}_i$ from target vector and continue for $i=n, \dots, 1$. After the last iteration $\sum_{i=1}^n  c_i \mathbf{b}_i$ is returned. 

Application to LWE: $\mathbf{t} = \mathbf{A}^\intercal\mathbf{s}+\mathbf{e}$ => we get $\mathbf{v}$ where $\mathbf{t}- \mathbf{v} = \mathbf{e}$ is in fundamental parallelipiped of Gram-Schmidt basis


Generalized version by \cite{LP11}:
Problem: in reduced basis last Gram-Schmidt vectors of B short, first long => long and skinny parallelipiped, Gaussian e unlikely to be in it => incorrect answer from NearestPlane

=> generalized version admitting time/success tradeoff
recurse on some $d_i \geq 1$ distinct planes in ith 

\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function}
  \Begin($\text{GeneralizedNearestPlane} {(} \mathbf{B} \in \mathbb{R}^{m \times k},\mathbf{t} \in \mathbb{R}^{m}, \mathbf{d} \in {(}\mathbb{Z}^+{)}^k {)}$) 
  { % TODO make sure that basis is always R^n, not Z^n
    \If{k = 0}{
      Return $\mathbf{0}$\\
    }
    \Else{
      Compute projection $\mathbf{v}$ of $\mathbf{t}$ onto $\text{span}(\mathbf{B})$\\
      Compute the $d_k$ distinct integers $c_1, \dots, c_{d_k}$ closest to $\langle \mathbf{v}, \tilde{\mathbf{b}}_k\rangle /  \langle \tilde{\mathbf{b}}_k, \tilde{\mathbf{b}}_k\rangle)$\\
      Return $\bigcup_{i \in \{1, \dots, d_k\}} (c_i \cdot \mathbf{b}_k + \text{GeneralizedNearestPlane}(\{\mathbf{b}_1, \dots, \mathbf{b}_{k-1}\}, (d_1, \dots, d_{k-1}), \mathbf{v} - c_i \cdot \mathbf{b}_k))$\\
    }
  }
\caption{Generalized Nearest Plane Algorithm \cite{LP11}}\label{alg:GeneralizedNearestPlane}
\end{algorithm2e}
Instead of choosing only the nearest plane in each iteration step, \cref{alg:GeneralizedNearestPlane} selects a variable amount $d_k$ of distinct planes in each step. As a consequence, the fundamental parallelipiped of the Gram-Schmidt basis is stretched in the direction of $\tilde{\mathbf{b}}_k$. The values of $\mathbf{d}$ should be chosen such that the covered area is approximately the same in each direction (i.e. by maximizing $\min_i(d_i \cdot \|\tilde{\mathbf{b}}_i\|)$). In particular this implies that the $d_k$ are larger for larger $k$ as the Gram-Schmidt vectors have a smaller length. % TODO combine with section above 
Compared to \cref{alg:babai} the runtime increases by a factor $\prod_{i \in \{1, \dots, d_k\}} d_i$, however, the recursion step can be fully parallelized.

It should be evident that a lower quality of the reduced input basis can be compensated for by increasing the values of $\mathbf{d}$. Hence we can adjust the input parameters for the lattice reduction and \cref{alg:GeneralizedNearestPlane} to minimize the runtime given a fixed required success probability. % TODO perhaps reformulate
% TODO: add exact success probability? 




\subsection{Primal-uSVP \cite{ADPS16, BG14}} % => AFG14, Kan87
% TODO: taken from ADPS16, change
BKZ: reduce lattice basis using SVP oracle in smaller dimension $b$, known that number of calls to oracle polynomial
- enumeration algorithm as oracle: in super-exponential time
- sieve algorithms as oracle: exponential time but so far slower in practice for accesible dimensions $b\approx 130$

primal attack: construct unique-SVP instance from LWE instance % TODO check if u-SVP is defined
LWE instance $(\mathbf{A}, \mathbf{z} = \mathbf{A}^\intercal \mathbf{s} + \mathbf{e})$
construct lattice 
\begin{equation}
  \Lambda = \left\{ \mathbf{x} \in \mathbb{Z}^{m+n+1} \mid (\mathbf{A}^\intercal | -\mathbf{I}_m | -\mathbf{b})\mathbf{x} = \mathbf{0} \mod q \right\} 
\end{equation}
lattice has dimension $d=m+n+1$, volume $q^m$ % TODO show
and unique-SVP solution $\mathbf{v} = (\mathbf{s}, \mathbf{e}, 1)$ % TODO show

success condition:
- geometric series assumption known to be optimistic from attacker's point of view
=> finds basis with Gram-Schmidt norms $\|\tilde{mathbf{b}}_i\| = \delta^{d - 2i-1} \cdot \text{Vol}(\Lambda)^{1/d}$ and $\delta = ((\pi b)^{1/b} \cdot b/2\pi e)^{1/2(b-1)}$ % see APS15 and Chen13 thesis
unique short vector $\mathbf{v}$ is detected if projection of $\mathbf{v}$ onto span of last $b$ Gram-Schmidt vectors is shorter than $\tilde{mathbf{b}}_{d-b}$, norm of projection is expected to be $\gamma \sqrt{b}$ => attack successful iff $\gamma \sqrt{b} \leq \delta^{d - 2i-1} \cdot q^{m/d}$
% TODO define span, e.g. vector space spanned by a number of vectors
% TODO check if einheitsmatrix is defined

LWE as inhomogeneous-SIS (ISIS)

% FROM APS15:
As in \cref{sec:decoding}, we view the LWE$_{n, q, m, \chi}$ instance $(\mathbf{A}, \mathbf{z})$ as a BDD instance in the  $q$-ary lattice $\Lambda(\mathbf{A}^\intercal) = \{ \mathbf{y} \mid \exists \mathbf{x} \in \mathbb{Z}_q^n : \mathbf{y} = \mathbf{A}^\intercal \mathbf{x}  \mod q \}$ \cref{sec:lwe-bdd} generated by rows of LWE instance. The target vector is $\mathbf{z}$. % TODO add to lwe as BDD/combine

Recall the $\gamma$-uSVP problem. Given a lattice $\Lambda$ where $\lambda_2(\Lambda) > \gamma \lambda_1(\Lambda)$, we are asked to find shortest nonzero vector in $\Lambda$. 
In the primal attack, instead of directly solving BDD, we reduce BDD to uSVP, i.e., we reduce a BDD instance to a $\gamma$-uSVP instance. By solving $\gamma$-uSVP, we obtain a solution to BDD. 
To do this we apply Kannan's embedding technique \cite{Kan87}. % TODO: quote theorem? For any $\gamma \geq 1, there is a polynomial time Cook-reduction from BDD$_{1/(2\gamma)}$ to $\gamma$-uSVP.
Intuitively, Kannan's embedding creates a lattice with uSVP structure. We know that $\mathbf{A}^\intercal \mathbf{s}\mod q$ is the closest vector to the target $\mathbf{z} =\mathbf{A}^\intercal \mathbf{s} + \mathbf{e}^\intercal \mod q$ in $\Lambda(\mathbf{A}^\intercal)$. We now add a linearly indpenendent basis vector  $(\mathbf{z}, \mu)$ and append a zero coefficient to each basis vector of the original lattice (i.e. the rows of $\mathbf{A}$). Thereby, we ensure that the new lattice contains the vector $[-\mathbf{e}, -\mu]^\intercal$ as $[\mathbf{A} \mid \mathbf{0}]^\intercal \mathbf{s} - 1 \cdot [\mathbf{z}^\intercal, \mu] = [-\mathbf{e}, -\mu]^\intercal$. 

More formally, let $\mathbf{B}$ be a basis of $\Lambda(\mathbf{A}^\intercal)$ % TODO: how? is \Lambda(\mathbf{A}^\intercal) not q-ary? => is A^\intercal not a basis?
and an embedding factor $\mu = \text{dist}(\mathbf{z}, \Lambda(\mathbf{A}^\intercal)) = \| \mathbf{z} - \mathbf{s}\|$ where $\mathbf{s}$ is the secret vector of the LWE instance. A relatively close approximation of $\mu$ can be guessed in polynomial time (see \cite{LM09} for more details). We now embed $\Lambda(\mathbf{A}^\intercal)$ into $\Lambda(\mathbf{B}')$ with $\gamma$-uSVP structure as follows: % TODO: check if dist = \| \mathbf{z} - \mathbf{s}\| is correct, perhaps just a vector v such that equation is fulfilled???
\begin{equation}
  \mathbf{B}' = \begin{pmatrix}
    \mathbf{B} & \mathbf{z}\\
    \mathbf{0}^\intercal & \mu
  \end{pmatrix}
\end{equation}
If $\gamma \geq 1$ and $\mu < \frac{\lambda_1(\Lambda(\mathbf{B})}{2\gamma}$ (or equivalently, $(\Lambda(\mathbf{A}^\intercal), \mathbf{z})$ a BDD$_{1/(2\gamma)}$-instance), then $\Lambda(\mathbf{B}')$ contains a $\gamma$-unique shortest vector $\mathbf{z}' = \left[(\mathbf{A}^\intercal\mathbf{s} - \mathbf{z})^\intercal, -\mu\right]^\intercal = \left[-\mathbf{e}^\intercal, -\mu\right]^\intercal$. 
The statement can be proven by showing by contradiction that all vectors $\mathbf{v} \in \Lambda{\mathbf{B}'}$ that are independent of $\mathbf{z}'$ satisfy $\| \mathbf{v}\| \geq \lambda_1{\Lambda(\mathbf{B})}/\sqrt{2} > \sqrt{2}\gamma \mu = \gamma \|\mathbf{z}'\|$ (see Section 4 of \cite{LM09} for more details). Note that the reduction can be done in polynomial time (Theorem 4.1 in \cite{LM09}).% TODO possibly sketch the proof
Hence, from $\mathbf{z}'$ we can recover the error vector $\mathbf{e}$ and thereby the secret vector $\mathbf{s} = \mathbf{z} - \mathbf{e} \mod q$. 

A solution to $\gamma$-uSVP can be found by reducing it to $\kappa$-HSVP where $\gamma = \kappa^2$ \cite{APS15}. Various algorithms, in particular, lattice reduction algorithms, exist to solve $\kappa$-HSVP. If we are able to solve a linear number of $\kappa$-HSVP instances that correspond to a $\kappa^2$-approximate SVP instance, we can construct a solution the latter (see \cref{def:gammaSVP}, see Section 1.2.21 in \cite{Lov87} for more details). 
Consider any lattice with uSVP structure. In exactly one direction, that is, in the direction of its unique shortest vector, the lattice has vectors that are significantly smaller than in other directions. A lattice reduction algorithm that yields a sufficiently good output basis quality, therefore, must return some small vector in the desired direction. 
Let $\mathbf{v}$ be a solution to SVP$_\kappa^2$, i.e. $\|\mathbf{v}\| \leq \kappa^2 \lambda_1(\Lambda)$. All other vectors $\mathbf{w}\in \Lambda$ that are not multiples of a shortest vector have length $\|\mathbf{w}\| \geq \lambda_2(\Lambda) > \kappa^2\lambda_1(\Lambda)$. Thus, we obtain a solution to $\gamma$-uSVP and, as shown above, we can reconstruct the secret vector to solve LWE. 

% TODO: in practice: GN08



\subsection{Meet-in-the-Middle \cite{APS15}}
% simple description in AFFP14 sec 6



\subsection{Arora-Ge \cite{AG11}}




\section{Algorithms for Solving SIS}
Recall that the SIS$_{n, q, m, \beta}$ problem asks to find a short vector $\mathbf{s} \in \mathbb{Z}_q^m$ of norm $\|\mathbf{s}\| \leq \beta$ such that such that $\mathbf{A} \cdot \mathbf{s} = 0 \; \text{mod } q$ for some uniformly distributed matrix $\mathbf{A}^{n\times m}$. Solving SIS is equivalent to finding a short vector in the dual lattice $\Lambda(\mathbf{A}^\intercal)^{\perp} = \{ \mathbf{y} \in \mathbb{Z}^m \mid \mathbf{A} \mathbf{y} = \mathbf{0} \mod q\}$. 


\subsection{Lattice Reduction \cite{MR09, RS10}}
\subsubsection{MR variant \cite{MR09}} \label{sec:mr-variant}
Our first approach to solving SIS follows quite naturally. Given $\mathbf{A}$, we can efficiently compute the basis $\mathbf{B} (\mathbf{B}^\intercal \mathbf{B})^{-1}$ of the dual lattice $\Lambda(\mathbf{A}^\intercal)^{\perp}$ in polynomial time using Gauss-Jordan elimination or other more modern algorithms.

We can then apply a lattice reduction algorithm and obtain a basis with root Hermite factor $\delta$. The first basis $\mathbf{b}_1$ vector of the reduced basis has length $\|\mathbf{b}_1\| = \delta^m \text{det}(\Lambda(\mathbf{A}^\intercal)^{\perp})^{1/m}$. We can see that $\delta$ depends on the subdimension $m$ which we want to be ideal in order to minimize the cost of the lattice reduction by relaxing $\delta$. 

We further assume that $\text{det}(\Lambda(\mathbf{A}^\intercal)^{\perp}) = \text{Vol}(\Lambda(\mathbf{A}^\intercal)^{\perp}) = q^n$ (see \cite{MR09} for more details). For $q$ prime and $m$ much larger than $n$ we have that the rank of $\mathbf{A}$ is $n$ as the rows of $\mathbf{A}$ are with high probability linearly independent. The nullity or the dimension of the kernel of $\mathbf{A}$ is $m-n$ and as a result the dual lattice has $q^{m-n}$ points in $\mathbb{Z}_q^m$. Consider the fundamental domain $D = \mathcal{P}(\Lambda(\mathbf{A}^\intercal)^{\perp})$ and the fact that $\Lambda(\mathbf{A}^\intercal)^{\perp} + (D \mod q) = \mathbb{R}^m/q\mathbb{R}^m$ is a partition. The volume of $\mathbb{R}^m/q\mathbb{R}^m$ is given by $q^m =|\Lambda(\mathbf{A}^\intercal)^{\perp}||D \mod q|$ and thus
\begin{equation}
  \text{det}(\Lambda(\mathbf{A}^\intercal)^{\perp}) = |D \mod q| = \frac{q^m}{|\Lambda(\mathbf{A}^\intercal)^{\perp}|} = \frac{q^{m}}{q^{m-n}} = q^n.
\end{equation}


We now have our first equation 
\begin{equation}\label{eq:mr-delta}
  \|\mathbf{b}_1\| = \delta^m q^{\frac{n}{m}},
\end{equation}
which becomes minimal for $m = \sqrt{n \log q / \log \delta}$.
\begin{theorem}[Optimal subdimension $m$ \cite{MR09}]
  Given a $q$-ary scaled dual lattice  $\Lambda(\mathbf{A}^\intercal)^{\perp}$ defined by a matrix $\mathbf{A} \in \mathbb{Z}^{n \times m}$ with $m$ sufficiently larger than $n$ and a prime $q$. Then a lattice reduction algorithm yields an optimal output if performed in subdimension 
  \begin{equation}
    m' = \sqrt{\frac{n \log q}{\log \delta}}. \label{eq:mr-m}
  \end{equation} 
\end{theorem}

Higher dimension increase the complexity of the reduction algorithms and lower dimensions may cause a lack of sufficiently short lattice vectors \cite{MR09}. In contexts in which \cref{eq:mr-delta} does not hold, we may still choose $m$ as in \cref{eq:mr-m} heuristically. Removing columns from $\mathbf{A}$ does not greatly impact our results since we can just set the corresponding components of the secret vector $s$ to zero. We reformulate \cref{eq:mr-delta} a bit:
\begin{align}
  \|\mathbf{b}_1\| = \delta^m q^{\frac{n}{m}} \iff&\log \beta = m \log \delta + \frac{n \log q}{m}\\
  \iff&\log \delta = \frac{\log \beta}{m} - \frac{n \log q}{m^2} \label{eq:mr-log-delta}
\end{align}

We continue by plugging \cref{eq:mr-m} into \cref{eq:mr-log-delta}:
\begin{align}
  \log \delta = \frac{\log \beta}{\sqrt{\frac{n \log q}{\log \delta}}} - \frac{n \log q}{\left(\sqrt{\frac{n \log q}{\log \delta}}\right)^2} \iff&\log \delta = \frac{\log \beta}{\sqrt{\frac{n \log q}{\log \delta}}} - \log \delta\\
  \iff&2\log \delta = \frac{\log \beta}{\sqrt{\frac{n \log q}{\log \delta}}}\\
  \iff&\log \delta = \frac{\log^2 \beta}{4n \log q}
\end{align}

\begin{theorem}[Optimal subdimension $m$ \cite{MR09}]
  Given a $q$-ary scaled dual lattice $\Lambda(\mathbf{A}^\intercal)^{\perp}$ defined by a matrix $\mathbf{A} \in \mathbb{Z}^{n \times m}$ with $m$ sufficiently larger than $n$ and a prime $q$. Then a lattice reduction algorithm performed in its optimal subdimension achieves a log root Hermite factor of  
  \begin{equation}
    \log \delta = \frac{\log^2 \beta}{4n \log q}. \label{eq:mr-log-RHF}
  \end{equation} 
\end{theorem}

To estimate the cost of the lattice reduction for SIS, we call a function from the \textit{Estimator} to find the required block size $k$ such that BKZ achieves root Hermite factor $\delta$ and apply a cost model with the optimal subdimension $m'$ and block size $k$. 
% TODO: describe how k is computed?

Note that for LWE we have $\alpha, q$ as input parameters instead of a bound. We can convert $\alpha$ to a required bound $\beta = \frac{1}{\alpha} \sqrt{\ln (\frac{1}{\epsilon})/ \pi}$ such that the success probability of solving an LWE instance is given by $\epsilon$ (Corollary 2 in \cite{APS15}). The \textit{Estimator} uses a rinse and repeat strategy to find the best tradeoff between runtime and success probability.

% TODO: fall unterscheidungen, m' > m oder delta zu klein





\subsubsection{RS variant \cite{RS10}}
% permissive form of distiguishing attack in \cite{MR09}, adversarial advantage is about $2^{-72}$ \cite{LP11}.

A similar approach is described in \cite{RS10}. The optimal subdimension and required root Hermite factor are given by a slightly different expression. Apart from that the attack works as described in \cref{sec:mr-variant}.

\begin{theorem}[Optimal subdimension $m$ (\cite{RS10}, Conjecture 2)]
  For every $n \geq 128,$ constant $c \geq 2, q \geq n^c, m = \Omega(n \log_2(q))$ and $\beta < q$, the best known approach to solve SIS with parameters ($n, m, q, \beta$) involves solving $\delta$-HSVP in dimension $m' = \min(x : q^{2n/x} \leq \beta)$ with $\delta = \sqrt{d}{\beta / q^{n/m'}}$.
\end{theorem}

We reformulate the expression for $m'$
\begin{align}
  q^{2n / m'} &\leq \beta \\
  \frac{2n}{m' \log(q)} &\leq \beta \\
  m' &\geq \frac{2n \log(q)}{\log(\beta)}
\end{align}
and obtain $m' = \left\lceil \frac{2n \log(q)}{\log(\beta)} \right\rceil$. If $m' > m$, we take $m' = m$. 

The root Hermite factor $\delta$ must be larger than $1$ for the reduction to be tractable. From $\delta = \sqrt{d}{\beta / q^{n/m'}} \geq 1$ it follows that we need $m' \geq n \log_2(q) / \log_2(\beta)$. 

% TODO: add some comments
% TODO: give intuition about the conjecture???


\subsection{Combinatorial Attack \cite{MR09}}
Micciancio and Regev also describe a combinatorial method for solving SIS \cite{MR09}. 

- matrix $\mathbf{A} \in \mathbb{Z}_q^{n \times m}$, dual lattice $\Lambda(\mathbf{A}^\intercal)^{\perp}$, find lattice vector bounded by $b$ in all $m$ coefficients

- $2^k$ sets of column vectors for some $k$, $m/2^k$ columns in each set
- compute all linear combinations $c_1 \mathbf{b}_1 + \cdots + c_{m/2^k}\mathbf{b}_{m/2^k}$ for column vectors $\mathbf{b}_i$ in each set such that the coefficients are bounded by $b$, i.e. $|c_1| \leq b$ to produce $2^k$ new sets of $l=(2b+1)^{m/2^k}$ vectors % TODO: at most or exactly?
- combine sets pairwisely: for each vector in first set $\mathbf{x}$ combine with each vector $\mathbf{y}$ in second set, if first $\log_q l$ components in $\mathbf{x} \pm \mathbf{y}$ are zero put in combined set
- combined sets have size $l$ as $\log_q l$ components $q^{\log_q l}$ values % TODO rewrite
- now $2^{k-1}$ sets, repeat $k-1$ times => vectors in last result set have zero entries in first $k \log_q l$ components % TODO

Choose $k$ such that 
\begin{equation}
  n \approx (k+1) \log_q l = (k+1) \log_q (2b+1)^{m/2^k} \iff \frac{2^k}{k+1} \approx \frac{m \log(2\beta + 1)}{n \log(q)}
\end{equation}
- then approximately $\log_q l$ coodinates in result set that are not cancelled out => we should obtain at least one zero vector as at there are most $l$ different vectors
- zero vector is linear combination with entries bounded by $b$


\begin{align}
  \frac{2^k}{k+1} &\approx \frac{m \log(2\beta + 1)}{n \log(q)}\\
  \text{diff} &= \text{abs}\left(\frac{2^k}{k+1} - \frac{m \log(2\beta + 1)}{n \log(q)}\right)
\end{align}

To find an optimal $k$, we iterate over $k$ starting from $k=1$ and calculate the difference $\text{diff}$. When $\text{diff}$ does not decrease for 10 iteration steps, we stop and take the current $k$.

We make a conservative estimate of the cost by estimating the number of operations needed to create the initial lists as the overall cost is dominated by this parameter. Each of the $2^k$ lists contains $L$ vectors. The cost for any operation on a list element is at least $\log_2(q) \cdot n$. Hence, the total cost is $2^k * L * \log_2(q) * n$.



% TODO maybe change definition of lattice so that we can write A instead of A^\intercal.