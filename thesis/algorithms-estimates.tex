
\section{Lattice Basis Reduction} % perhaps move to Algorithms
% For survey see Ngu11, NV10, MW16

% TODO: rewrite (taken from AGVW17)
Problem: usually ugly basis (long vectors...), we want a better basis with shorter and more orthogonal basis vectors...
- improve lattice basis quality => measure by hermite factor (compare shortest vector in basis to lattice volume) or approximation factor (compare shortest vector in basis to shortest lattice vector)
- algorithm finding vector with approximation factor $\gamma$ can be used to solve uSVP with gap $\lambda_2(\Lambda)/\lambda_1(\Lambda) > \gamma $
- best known theoretical bound by Slide reduction \cite{GN08a}, BKZ better in practice

% TODO possibly put Gram-Schmidt orthogonalization here? => Size reduction algorithm and HKZ reduction, see 2.4.1 in Chen13, not sure if needed
- measure quality of basis: Hermite factor  % TODO change or \cite{Reg10}

  * basis $\mathbf{B} = \left\{\mathbf{b}_1, \ldots, \mathbf{b}_m\right\}$, $m$-dimensional lattice $\Lambda(\mathbf{B})$ has Hermite factor $\delta$ if
  \begin{equation} \label{eq:hermite}
    \| \mathbf{b}_1 \| \approx \delta^m \det(\Lambda)^{1/m}
  \end{equation}

  * use Geometric Series Assumption (GSA) to obtain estimates for $\mathbf{b}_i$: % TODO: necessary?
    \begin{equation} \label{eq:GSA}
      \| \tilde{\mathbf{b}}_i \| \approx \alpha^{i-1} \| \mathbf{b}_1 \|
    \end{equation}
    for $0 < \alpha < 1$
    \cref{eq:hermite} into \cref{eq:GSA} -> $\| \tilde{\mathbf{b}}_i \| \approx \alpha^{i-1} \delta^m \det(\Lambda)^{1/m}$
    with $\prod_{i-1}^m \| \tilde{\mathbf{b}}_i \| = \det(\Lambda)$ we get 
    \begin{align*}
      &\quad& \prod_{i-1}^m \| \tilde{\mathbf{b}}_i \| &\approx \prod_{i-1}^m \alpha^{i-1} \delta^m \det(\Lambda)^{1/m} \\
      \Leftrightarrow&\quad& \det(\Lambda) &\approx \delta^{2m} \det(\Lambda) \prod_{i-1}^m \alpha^{i-1}\\
      \Leftrightarrow&\quad& \delta^{-m^2}  &\approx \alpha^{\frac{m(m-1)}{2}}\\
      \Leftrightarrow&\quad& \delta^{-2}  &\approx \alpha^{(m-1)/m}\\
    \end{align*}
    Hence, $alpha \approx \delta^{-2}$ and 
    \begin{equation}
      \| \tilde{\mathbf{b}}_i \| \approx \delta^{-2(i-1) + m} \det(\Lambda)^{1/m}
    \end{equation}

  * good basis -> first Gram-Schmidt vectors become shorter (latter longer)

  * $\delta = 1.01$ feasible, $\delta = 1.007$ seems infeasible for now

  * gap between provable and experimental cost estimate to reach some hermite $\delta$ => provable results only give upper bounds, for practical security we need lower bound => combine theoretical results with experimental results

  * well-established estimate \cite{LP11}

In the following, we will focus on two related methods for lattice reduction. % A third approach, the Hermite, Korkine, Zolotarev (HKZ) reduction %TODO: write a sentence or two about it?

% TODO: algorithm LLL reduction, check out CWX13
\subsection{LLL}
The LLL algorithm was proposed by Lenstra, Lenstra and Lovász \cite{LLL82} and can be considered as a generalization of the two dimensional Lagrange reduction. The lagrange reduction reduces a basis of two basis vectors such that output basis satisfies $\|\mathbf{b}_1\| \leq \|\mathbf{b}_2\|$ and $\frac{|\left\langle\mathbf{b}_1, \mathbf{b}_2\right\rangle|}{\|\mathbf{b}_1\|} = |\mu_{2,1}| \leq \frac{1}{2}$). Intuitively, a multiple of the shorter vector $\mathbf{b}_1$ is subtracted from the longer vector $\mathbf{b}_2$ such that the resulting vector $\mathbf{b}_2'$ is as orthogonal to $\mathbf{b}_0$ as possible, i.e.  $\mathbf{b}_1' =  \mathbf{b}_1 - \lfloor\mu_{1,0}\rceil \mathbf{b}_0$. We set $\mathbf{b}_2 = \mathbf{b}_2'$ and repeat until nothing changes. 

A $\delta$-LLL reduced basis ensures two criterias \cite{LLL82}:
\begin{enumerate}
  \item Size reduced: $|\mu_{i,j}| \leq \frac{1}{2}$ for $1\leq i \leq n$ and $j < i$ \label{size-red}
  \item Lovász condition: $\delta \| \tilde{\mathbf{b}}_i \|^2 > \| \mu_{i+1, i} \tilde{\mathbf{b}}_i + \tilde{\mathbf{b}}_{i+1} \|^2$ for $1\leq i < n$
\end{enumerate}

Recall the definition of the Gram-Schmidt coefficients $\mu_{i, j} = \frac{\left\langle \tilde{\mathbf{b}}_j, \mathbf{b}_i\right\rangle}{\left\langle \tilde{\mathbf{b}}_j, \tilde{\mathbf{b}}_j\right\rangle}$. The LLL algorithm shown in \cref{alg:LLL} follows the notation in \cite{LLLReg04}. We start by computing the Gram-Schmidt orthogonalization of the input basis (\cref{alg:LLL-start}) and continue with a reduction step in which we update every basis vector $\mathbf{b}_i$ by pairwisely comparing and subtracting lower indexed basis vector just as in the Lagrange reduction (\cref{alg:LLL-red}) to ensure Criteria \ref{size-red}. Finally, vectors violating the Lovász condition are swapped (\cref{alg:LLL-swap}ff) and the process is repeated until nothing changes. The LLL algorithm can be used to find short vectors of at most $2^{n/2} \lambda_1(\Lambda)$ in polynomial time. Several floating-point variants have been suggested that can significantly speed up the runtime of LLL. For example, L$^2$ runs in $O\mathcal{O}(n^2 \log^2 B)$, where $B$ is a bound on the norm of the input basis vectors \cite{NS05}. % TODO: newer versions?

\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function} % FROM https://cims.nyu.edu/~regev/teaching/lattices_fall_2004/ln/lll.pdf
\Begin($\delta\text{-LLL} {(}\mathbf{B} \in \mathbb{Z}^{m\times n} {)}$) % TODO nxn???
{
  Compute $\tilde{\mathbf{B}}$\label{alg:LLL-start}\\
  \For{$i=2, \dots, n$}{ 
    \For{$j=i-1, \dots, 1$}{
      $\mathbf{b}_i = \mathbf{b}_i - \lfloor\mu_{i, j}\rceil \mathbf{b}_j$\label{alg:LLL-red}\\
    }
  }
  \If{$\exists i \text{ \upshape such that } \delta \| \tilde{\mathbf{b}}_i \|^2 > \| \mu_{i+1, i} \tilde{\mathbf{b}}_i + \tilde{\mathbf{b}}_{i+1} \|^2$}{\label{alg:LLL-swap}
    tmp = $\mathbf{b}_i$\\
    $\mathbf{b}_{i} = \mathbf{b}_{i+1}$\\
    $\mathbf{b}_{i+1} = \mathbf{b}_{i}$\\
    Return $\delta$-LLL($\mathbf{B}$)\\
  }
  \Else{
    Return $\mathbf{B}$\\
  }
}
\caption{The $\delta$-LLL Algorithm} \label{alg:LLL}
\end{algorithm2e}



\subsection{BKZ}
% Rewrite, from Pla18

The Block Korkin-Zolotarev (BKZ) algorithm was proposed by Schnorr in 1987 and adapted by Schnorr and Euchner in \cite{SE91} and represents a family of lattice reduction algorithm. Essentially, BKZ iteratively divides the input basis into blocks of a lower dimension $k$ and calling an SVP oracle on each block. The output of the oracle is then used to obtain a basis of improved quality. 

\cref{alg:BKZ} presents the main concept of BKZ and follows the description in \cite{CN11} with some adjustments. Initially, we run an LLL reduction on the input basis $\left\{\mathbf{b}_1, \dots, \mathbf{b}_{n}\right\}$ and update the basis. In each $j$th iteration, we consider a block of $k$ basis vectors $\mathbf{b}_j, \dots, \mathbf{b}_{j+k-1}$. The vectors of the current block are projected onto the orthogonal complement of the span of vectors from previous iterations $\text{span}\left(\left\{\mathbf{b}_i | i \in [j-1]\right\}\right)$ (\cref{BKW-span, BKW-proj}, we skip this step if the span is empty). Note that the orthogonal complement $A^\perp$  of a subspace $A$ is defined as the set of all vectors that are orthogonal to every vector in $A$. We then run an SVP oracle on the projected block to obtain a shortest vector $\mathbf{b}_\text{new}'$ in the projected lattice (\cref{BKZ-svp}) and reconstruct a lattice vector $\mathbf{b}_\text{new}$ of which $\mathbf{b}_\text{new}'$ is a projection \cref{BKZ-rec}. Note that in practice, the SVP oracle should include this step. If $\mathbf{b}_\text{new}$ is a new vector we insert it in our list of basis vectors before $\mathbf{b}_j$. Otherwise as nothing changed, we increment a counter $z$. Finally, we run LLL on all basis vectors up to index $j+i$ (including the possibly newly added vector). If no new lattice vectors can be found in $n$ iterations, the reduction terminates. After $n$ iterations, $j$ is reset to start over at the first block. The ouput of the algorithm is a BKZ$_k$-reduced basis. For $k=2$ we obtain an LLL-reduced basis in polynomial time and for $k=n$ an optimally HKZ-reduced basis in at least exponential time.%TODO needed? probably not, maybe just mention k=2 case, Pla18


\begin{algorithm2e} % TODO: check if block is not of dimension kxk
  \SetKwBlock{Begin}{function}{end function}
  \Begin($\text{BKZ} {(} \mathbf{B} = \{\mathbf{b}_1, \dots, \mathbf{b}_{n}\}, k \in [n]\backslash\{1\} {)}$) % TODO nxn???
  {
    z = 1; j = 0\\
    $\mathbf{B} = \text{LLL}(\mathbf{B})$ \\% LLL reduce and update basis
    \While{$z < n-1$}{ 
      $j = (j \mod (n-1)) + 1; l = \min(j + k -1, n); h = \min(l + 1, n)$\\
      $A = \text{span}\left(\left\{\mathbf{b}_i | i \in [j-1]\right\}\right)$\label{BKZ-span}\\
      \For{$i \in \{j, ..., l\}$}{
        \If{$A \neq \emptyset$}{
          $\mathbf{b}_i' = \pi_{A^\perp}(\mathbf{\mathbf{b}_i})$\label{BKZ-proj}\\
        }
        \Else{
          $\mathbf{b}_i' = \mathbf{b}_i$\\
        }
      }     
      $\mathbf{b}_\text{new}' = \text{SVP-Oracle}(\mathbf{b}_j', \dots, \mathbf{b}_{l}')$\label{BKZ-svp}\\
      Reconstruct $\mathbf{b}_\text{new} = \textstyle \sum_{i=j}^l\alpha_i \mathbf{b}_i$ with $\alpha_i \in \mathbb{Z}$ such that $\mathbf{b}_\text{new}' = \pi_{\left(\text{span}\left(\mathbf{b}_j, \dots, \mathbf{b}_{l}\right)\right)^\perp}(\mathbf{b}_\text{new})$\label{BKZ-rec}\\
      \If{$\mathbf{b}_\text{new}' \neq \tilde{\mathbf{b}}_j$}{
        $z=0;$ $\left\{\mathbf{b}_j, \dots, \mathbf{b}_{h} \right\}= \text{LLL}(\left\{\mathbf{b}_j, \dots, \mathbf{b}_{j-1}, \mathbf{b}_\text{new}, \mathbf{b}_j, \dots, \mathbf{b}_{h} \right\})$\\
      }
      \Else{
        $z = z + 1;$ $\left\{\mathbf{b}_j, \dots, \mathbf{b}_{h} \right\} = \text{LLL}(\left\{\mathbf{b}_j, \dots, \mathbf{b}_{h} \right\})$\\
      }
    }
  }
  \caption{The BKZ Algorithm} \label{alg:BKZ}
\end{algorithm2e}

% TODO: explain how SVP oracle can be implemented => possibly outsource, section for cost models!!!

Several improvements have been suggested (see \ref{fig:BKZ-improvements}). The total number of rounds until termination is unknown and can be quite large. Hanrot \textit{et al.} \cite{HPS11a} show an early termination of BKZ still yields a very good output basis quality and propose $\frac{n^2}{k^2} \log n$ rounds as a bound. Th
Local preprocessing increases the quality of the current block basis by recursively calling BKZ with smaller block size. A variant known as progressive BKZ applies the recursion globally \cite{AWHT16}. If enumeration is used as an SVP oracle (see \cref{sec:costmodels}), extreme pruning can be applied to significantly reduce the search space. Gamma \textit{et al.} show that applying such a bounding function on the search tree reduces the running time by a much larger factor than the success probability. Repeating the search yields the desired speedup \cite{GNR10}. In addition, \cite{CN11} optimizes the enumeration radius by using experimental results. BKZ 2.0 incorporates a number of these techniques \cite{CN11}.

% BKZ runtime bounds: TODO rewrite, from MW16
% - runtime (without early termination) grows superpolynomially in $n$ \cite{GN08b}
% - best provable bounds on ouput after termination worse than that of slide reduction by at least polynomial factor (slide reduction [\cite{GN08a}] yields best theoretical results but in practice worse than BKZ)
% - calls to SVP oracle in arbitrary dimensions up to block size => runtime depends on worst-case Hermite constants of each block size 
% => simulation of BKZ needs to include estimation of runtime of SVP oracle for each block size
% TODO: use the following? 
% Versions:
% - slide reduction yields best theoretical result (2^n log log n /log n) \cite{GN08a} 
% - practical floating point version of LLL \cite{SE91} best practical results 

It is difficult to find hard runtime bounds for BKZ. The upper bound on the number of rounds is superexponential in the dimension $n$ for a fixed block size \cite{HPS11a, GN08b} before BKZ terminates by itself. In addition, calls to the SVP oracle is called in all dimensions $k' \leq k$ must be taken into account. 
\citet{APS15} ignore these intricacies and estimate the cost of BKZ in clock cycles as $\rho \cdot n \cdot t_k$ where $\rho$ is the number of rounds needed and $t_k$ is the cost (in block cycles) of calling the SVP oracle on a block of dimension $k$. The value $\rho$ is set to $8$ in the \textit{Estimator} based on experiments in \cite{Chen13} that indicate that the most significant progress happens in the first $7-9$ rounds. 

% TODO: include? \cite{LP11} runtime according Linder-Peikert model, GSA: suggest $t_{\text{BKZ}}=1.8/\log_2{\delta} - 110$




- CN11, APS15, ADPS16
sieving \cite{BDGL16, Laa15,ADPS16, APS15,BDGL16}
enumeration \cite{Kan87, MW15,FP85, CN11, APS15, HPSSWZ17}
cost models (tabular overview) \cite{ACCD+19}
- number of SVP calls \cite{ADPS16,Alb17}
- upper bound on rounds is exponential \cite{HPS11a, GN08b}
- lattice rule of thumbs: achievable root hermite factor $\delta_0 = k^{\frac{1}{2k}} $ % [Stehlé12 An introduction to lattice reduction] or 2^1/k [Stehlé13/16  An overview of lattice reduction algorithms] (oversimplification)


\subsection{Cost Models for Lattice Reduction} \label{sec:costmodels}
% TODO. write little introduction
In this section, we will look at various high level ideas to realize an SVP oracle, i.e. to solve SVP, and present up-to-date cost models from the literature. 

SVP is NP-complete % TODO formulate, find source

% nice overview in https://cseweb.ucsd.edu/~daniele/LatticeLinks/Enum.html
- enumeration => \cite{ABFKSW20}, \cite{ABLR21}
enumerating all lattice vectors within a bounded region % TODO how?
low memory cost (linear in $n$), perform good in moderately low dimensions
provide asymtotically fastest algorithms in PSPACE both in theory and practice % TODO check
worst-case $2^{\mathcal{O}(n \log n)}$


- sieving => \cite{ADHKPS19}, \cite{AGPS20}
  \cite{ABFKSW20}: achieve root Hermite factor $k^{1/(2k)}$ in $k^{k/8 + o(k)}$ time and polynomial memory
  single exponential time algorithms $2^{cn}$, $c \in \mathcal{O}(1)$ % TODO change
  use randomization and exponential spaces
    
  We start by sampling a list of $2^{cn}$ lattice points $\mathbf{v}_i, i \in [2^{cn}]$ of length smaller than some bound $r$. Then, we choose some center points $\mathbf{v}'_i, i \in [l]$ for some $l \ll 2^{cn}$ of the list such that all points $\mathbf{v}_i$ of the initial list are covered by spheres of a radius $r' < r$ centered at these center points $\mathbf{v}'_i$. Finally, we compute short vectors by subtracting the centers. 

  primarily studied for SVP in euclidean norm
  two main types: 
  Classic Sieve \cite{AKS01}: create a long list of lattice vectors, then find shorter lattice vectors and discard longer
  List Sieve \cite{MV10}: start from empty list, find shorter vectors and append them to list

  best provable: $2^{n + o(n)}$ (sieving by averages)
  heuristic state of the art: $(3/2)^{n/2 + o(n)} \approx 2^{0.29n}$ 
  
  combinarial: create list of short random (possibly non-lattice) vectors, try to combine vectors in list to produce short lattice vectors
  provable versions not very useful (exponential space, random perturbations) but basis of heuristic variants used in practice 

  % add timeline?
  \begin{chronology}[5]{2000}{2021}{3ex}[\textwidth]
    \event{2001}{$2^{\mathcal{O(n)}}$ time and space \cite{AKS01}}
    \event{2004}{$2^{16n}$ time and $2^{8n}$ space \cite{Reg}}
    \event{2004}{$2^{16n}$ time and $2^{8n}$ space}
    \event{\decimaldate{25}{12}{2001}}{three}
  \end{chronology}


- Micciancio-Voulgaris Algorithm uses voronoi cell \cite{MV10} in $4^{n + o(n)}$ time and $2^{n+o(n)}$ space

- discrete Gaussian sampling \cite{ADRS15}
% TODO insert a table and reference somewhere else? Warum notwendig, wie kommt man darauf? ... alg laufen lassen, extrapolieren...




\section{Algorithms for Solving LWE}
\subsection{Three Approaches for Solving LWE}
% TODO from LP11, change
Distinguishing attacks (MR09, RS10): distinguish (with noticeable advantage) LWE instance from uniformly random => break semantic security of LWE-based cryptosystem with same advantage (typically), find short nonzero integral vector $\mathbf{v}$ s.t. $\mathbf{A}^\intercal \mathbf{v} = 0 \mod q$ => short vector in (scaled) dual of LWE lattice $\Lambda(\mathbf{A})$ % for A mxn matrix else switch t
then test whether $\langle \mathbf{v}, z \rangle$ is close to zero mod q. If uniform test accepts with prob 1/2, if LWE with parameter s, $\langle \mathbf{v}, \mathbf{z} \rangle = \langle \mathbf{v}, \mathbf{e} \rangle \mod q$, Gaussian mod q with parameter $\| \mathbf{v} \| \cdot s$. If that's not much larger than q, advantage for distinguishing very close to $\exp(-\pi (\| \mathbf{v} \| s/q)^2)$. high confidence needs $\| \mathbf{v} \| \leq q/(2s)$ 
advantage an computational effort need to be balanced (often inverse distinguising advantage is in total cost of attack)

\subsubsection{SIS}
rewrite LWE as the problem of finding short vector in dual lattice => SIS
\subsubsection{BDD}

lattice reduction algorithms solve SIS and BDD 
\subsubsection{Direct}

% TODO: based on GSJ15, rephrase
- algebraic approach Arora and Ge with subexponential complexity when $\sigma \leq \sqrt{n}$, else fully exponential, mainly of asymptotic interest (higher complexity than others)

- combinatorial algorithms: BKW as basis \cite{BKW03}, resembles generalized birthday approach by Wagner, % Wagner, D.: A generalized birthday problem. In: Advances in cryptologyCRYPTO 2002, pp. 288304. Springer (2002)
originally for solving LPN, can be analyzed => explicit complexity for different LWE instances, theoretical analysis and actual performance close,
very memory expensive (often same order as time complexity) 

\subsection{BKW \cite{BKW03}}

- BKW by Blum, Kalai and Wasserman \cite{BKW03} to solve Learning Parity with Noise problem (LPN), subproblem of LWE
- applied to LWE in \cite{ACFFP15a} (original paper appeared in 2013)
- time and space complexity $2^{\mathcal{O}(n)}$ for LWE with prime modulus $q \in \text{poly}(n)$ % TODO check n or m 
- Various improvements have been suggested since. For example, \cite{AFFP14} and \cite{KF15} use modulus switching for binary-LWE and other small secret variants \cite{AFFP14} and \cite{DTV15} applies multidimensional Furier transformations. % TODO define binary LWE \cite{AFFP14} and find others, what is modulus switching, how do they apply FFT?

modified BKW step -> coded-BKW step to cancel out more positions in the $\mathbf{a}$ vectors than traditional BKW step

map part of $\mathbf{a}$ vector into nearest codeword in lattice code (linear code over $\mathcal{Z}_q$, Euclidean distance)

introduces some noise, can be kept small by appropriate parameters

pair of $\mathbf{a}$ vectors map to same codeword => add together to create new sample with part of $\mathbf{a}$ vector cancelled

samples are input to next step in BKW procedure

additional steps using discrete FFT

slightly modified for BINARY-LWE (secret vector uniformly chosen from $\{0, 1\}^n$) greatly increases performance

\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function}
\Begin($\text{BKW} {(}\mathbf{A},\mathbf{z}, t{)}$)
{
  $i = 1$\\
  $\mathbf{A}^{(i)} = \mathbf{A}$\\
  $\mathbf{z}^{(i)} = \mathbf{z}$\\
  \While{the last $t$ coefficients of the columns of $\mathbf{A}^{(i)}$ are nonzero}{ % TODO
    // BKW step\\
    $j = 1$\\
    $\mathbf{T}^{(i)} = []$ \Comment Collision table\\
    \For{$k = 1, \ldots, m^{(i)}$}{
      // $m^{(i)}$ is number of columns in $\mathbf{A}^{(i)}$\\
      \uIf{last $(i\cdot b)$ coefficients of $\mathbf{a}_k^{(i)}$ are zero}{
        $\mathbf{a}_j^{(i+1)} = \mathbf{a}_k^{(i)}$\\
        $z_j^{(i+1)} = z_k$\\
        $j = j + 1$\\
      }
      \uElseIf{no match for $\mathbf{a}_k^{(i)}$ in $\mathbf{T}$}{
        $\mathbf{T} = \mathbf{T} + \left[\mathbf{a}_k^{(i)}\right]$ \Comment append to collision set
      }
      \uElseIf{match $\mathbf{a}_l^{(i)}$ for $\mathbf{a}_k^{(i)}$ is found}{
        \uIf{$\mathbf{a}_l^{(i)}$ matches $\mathbf{a}_k^{(i)}$ in the last $(i\cdot b)$ components}{
            $\mathbf{a}_j^{(i+1)} = \mathbf{a}_k^{(i)} - \mathbf{a}_l^{(i)}$; \Comment last $i \cdot b$ coefficients of $\mathbf{a}_j^{(i+1)}$ are now zero\\
            $z_j^{(i+1)} = z_k^{(i)} - z_l^{(i)} = y_j^{(i)} + e_j^{(i)}$, where $y_j^{(i)} = \left\langle \mathbf{s}, \mathbf{a}_j^{(i)}\right\rangle$ and $e_j^{(i)} = e_k^{(i)} - e_l^{(i)}$ \label{alg:BKW-z1}\\ 
            $j = j + 1$\\
        }
        \uElseIf{the negation of $\mathbf{a}_l^{(i)}$ in $\mathbb{Z}_q^n$ matches $\mathbf{a}_k^{(i)}$ in the last $(i\cdot b)$ components}{
          $\mathbf{a}_j^{(i+1)} = \mathbf{a}_k^{(i)} + \mathbf{a}_l^{(i)}$\\
          $z_j^{(i+1)} = z_k^{(i)} + z_l^{(i)} = y_j^{(i)} + e_j^{(i)}$, where $y_j^{(i)} = \left\langle \mathbf{s}, \mathbf{a}_j^{(i)}\right\rangle$ and $e_j^{(i)} = e_k^{(i)} + e_l^{(i)}$\label{alg:BKW-z2}\\
          $j = j + 1$\\
        } % TODO: maybe put both cases into one step?
      }
    }
    $i = i + 1$\\
    // Calculate input for next BKW step\\
    $\mathbf{A}^{(i)} = (\mathbf{a}_1^{(i)} \cdots \mathbf{a}_{j-1}^{(i)})$\\
    $\mathbf{z} = (z_1^{(i)}, \ldots, z_{j-1}^{(i)})$\\
  }
}
\caption{BKW}\label{alg:BKW}
\end{algorithm2e} % TODO check

In the following, we present an outline of the original BKW algorithm. The steps in \cref{alg:BKW} are inspired by the textual description in \cite{GJS15} with minor adjustments in notation. 

For the algorithm, we use the matrix notation of LWE as in \cref{eq:lwe-decoding}, i.e. $\mathbf{z} = \mathbf{A}^\intercal \mathbf{s} + \mathbf{e}$. BKW consists of a series of BKW steps that iteratively reduce the dimension of input matrix $\mathbf{A}$ by finding collisions of its column vectors in the currently examined block of $b$ entries. We start from the last $b$ entries of $\mathbf{A}^{(1)} = \mathbf{A}$. In every step $i$, we maintain a collision table $\mathbf{T}^{(i)}$ and loop over the columns $\mathbf{a}_k^{(i)}$ of $\mathbf{A}^{(i)}$ and distinguish between the following cases: (1) If $\mathbf{a}_k^{(i)}$ only has zero entries in the examined block, pass $\mathbf{a}_k^{(i)}$ and $z_k^{(i)}$ to the next step, (2) if no match of $\mathbf{a}_k^{(i)}$ or the negation of $\mathbf{a}_k^{(i)}$ can be found in the collision table, add $\mathbf{a}_k^{(i)}$ to the colission table, and (3) if a match $\mathbf{a}_l^{(i)}$ is found, compute $\mathbf{a}_l^{(i)} + \mathbf{a}_k^{(i)}$ or in the case of a negation match $\mathbf{a}_l^{(i)} - \mathbf{a}_k^{(i)}$ (in $\mathbb{Z}_q$) such that the last $b$ nonzero entries cancel out. By exploiting the symmetry of $\mathbb{Z}_q$ in this way, in every step we obtain at most $(q^b - 1)/2$ columns with distinct coefficients in the current $b$ entries. We also make note of ``observed symbols'' $z_j^{(i)}$ that represent the combination of two samples given their respective matching columns (see lines \ref{alg:BKW-z1}, \ref{alg:BKW-z2} for more details).

In each BKW step, the number of columns (and samples) decreases by at least $(q^b - 1)/2$ (size of the colusion set) and the variance of the error distribution $\sigma^2$ increases by a factor of two. Once the number of remaining nonzero rows of $\mathbf{A}$ is small enough, the remaining part of the secret vector $\mathbf{s}$ is guessed. A hypothesis test ensures that the remaining samples follow a Gaussian with noise $2^t\cdot \sigma^2$, where $t$ is the number of steps. Finally, back substitution is applied to obtain the complete secret vector $\mathbf{s}$. % TODO more detail? see DTV15

\subsubsection*{Coded-BKW \cite{GJS15}}
- change BKW step -> more column entries are removed, but additional noise
- index set $I$, $\mathbf{x}_I$ is part of $\mathbf{x}$ with entries indexed by $I$
- step $i$: $I$ set of $b$ positions to be removed, fix some $q$-ary linear $\left[N_i, b\right]$ code $\mathcal{C}_i$ with $q^b$ codewords, find the closest codeword $\mathbf{c}_I \in \mathcal{C}$ for every input vector $\mathbf{a}_I$ such that $\mathbf{a}_I = \mathbf{c}_I + \mathbf{e}_I$, where the error part $\mathbf{e}_I \in \mathbb{Z}_q^{N_i}$ is minimized by a decoding procedure.

Finally, we subtract two vectors and their correpsonding samples and pass the result to the next BKW step. Consider the inner product$\left\langle \mathbf{s}_{I}, \mathbf{a}_{I} \right\rangle = \left\langle \mathbf{s}_{I}, \mathbf{c}_{I} \right\rangle + \left\langle \mathbf{s}_{I}, \mathbf{e}_{I} \right\rangle$. In the subtraction, only the error part $\left\langle \mathbf{s}_{I}, \mathbf{e}_{I} \right\rangle$ remains. 
% TODO

% TODO: pre/post-processing



\subsection{Dual Attack [MicReg09]}
"Gama and Nguyen \cite{GN08b}: (in)feasibility of obtaining various Hermite factors
natural distinguishing attack on LWE by finding one relatively short vector in associated lattice"
% TODO: check out 6.4 in ADPS16
% TODO: main description in SIS section

\subsection{Decoding Attack \cite{LP11}} \label{sec:decoding}

combines lattice basis reduction followed by an enumeration algorithm (bounded-distance decoding with preprocessing?) => time/success tradeoff
specifically for LWE, exploits structural properties of LWE
on search version of LWE problem, approach preferable to distinguishing attack on decision LWE in \cite{MR09, RS10}, same or better advantage than distinguishing attack using lattice vectors of lower quality => runtime is smaller
post-reduction: simple extension of Babai's ``nearest-plane'' algorithm \cite{Bab85} % TODO describe
=> trade basis quality against decoding time
related to Klein's (de)randomized algorithm \cite{Kle00} for bounded-distance decoding

use entire reduced basis, post-reduction part is fully parallelizable
% prerequisites: babai's nearest plane algorithm (at least describe textually?), fundamental parallelipiped P_1/2



% TODO: algorithm Babai's NearestPlane % FROM https://cims.nyu.edu/~regev/teaching/lattices_fall_2004/ln/lll.pdf
LLL reduction to input Lattice, integer combination of basis vectors close to target (like inner loop in reduction step of LLL), seek vector in lattice close to target, finds output that is in fundamental parallelipiped $\mathcal{P}(\mathbf{B})$ \cref{eq:fundamental-parallelipiped} => if error vector not in $\mathcal{P}(\mathbf{B})$, secret is not restored % TODO
=> basis quality has to be sufficiently good
\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function}
\Begin($\text{NearestPlane} {(}\mathbf{B} \in \mathbb{R}^{m \times n},\mathbf{t}\in \mathbb{R}^{m}{)}$)
{
  run $\delta$-LLL on basis $\mathbf{B}$ with $\delta=\frac{3}{4}$\\ % TODO put outside of algorithm
  $\mathbf{b} = \mathbf{t}$\\
  \For{$i = n, \dots, 1$}{
    $c_i = \text{round}(\langle \mathbf{b}, \tilde{\mathbf{b}}_i\rangle /  \langle \tilde{\mathbf{b}}_i, \tilde{\mathbf{b}}_i\rangle)$
    $\mathbf{b} = \mathbf{b} - c_i \mathbf{b}_i$ 
  }
  output $\mathbf{t} - \mathbf{b}$
}
\caption{Babai's Nearest Plane Algorithm \cite{Bab85}}\label{alg:babai} % TODO change algorithm to match below or just take intuitive description
\end{algorithm2e}

Output is a lattice vector $\mathbf{v} \in \Lambda(\mathbf{B})$ such that $\|\mathbf{v} - \mathbf{t}\| \leq 2^{n/2} \text{dist}(\mathbf{t}, \Lambda(\mathbf{B}))$ % define distance to lattice, change 

Goal: recover lattice vector relatively close to target vector
Intuition:
 - project $\mathbf{t}$ to $\text{span}(\mathbf{B})$
 - from $i=n, \dots, 1$ find closest hyperplane $c_i \tilde{\mathbf{b}}_i + \text{span}(\mathbf{b}_1, \dots, \mathbf{b}_i)$ to the projection, subtract $c_i \mathbf{b}_i$ from the projection and continue % a little more detail
 - output vector is $\sum_{i=1}^n c_i \mathbf{b}_i$
% TODO: better description \as in BBDFGM14
for every basis vector $\mathbf{b}_i$ find $c_i$ such that distance between target and hyperplane spanned by $\mathbf{b}_1, ..., \mathbf{b}_{i-1}$ and shifted by $c_i \tilde{\mathbf{b}}_i$  is minimal % by using Gram-Schmidt vector
, subtract $c_i \mathbf{b}_i$ from target vector and continue for $i=n, \dots, 1$. After the last iteration $\sum_{i=1}^n  c_i \mathbf{b}_i$ is returned. 

Application to LWE: $\mathbf{t} = \mathbf{A}^\intercal\mathbf{s}+\mathbf{e}$ => we get $\mathbf{v}$ where $\mathbf{t}- \mathbf{v} = \mathbf{e}$ is in fundamental parallelipiped of Gram-Schmidt basis


Generalized version by \cite{LP11}:
Problem: in reduced basis last Gram-Schmidt vectors of B short, first long => long and skinny parallelipiped, Gaussian e unlikely to be in it => incorrect answer from NearestPlane

=> generalized version admitting time/success tradeoff
recurse on some $d_i \geq 1$ distinct planes in ith 

\begin{algorithm2e}
\SetKwBlock{Begin}{function}{end function}
  \Begin($\text{GeneralizedNearestPlane} {(} \mathbf{B} \in \mathbb{R}^{m \times k},\mathbf{t} \in \mathbb{R}^{m}, \mathbf{d} \in {(}\mathbb{Z}^+{)}^k {)}$) 
  { % TODO make sure that basis is always R^n, not Z^n
    \If{k = 0}{
      Return $\mathbf{0}$\\
    }
    \Else{
      Compute projection $\mathbf{v}$ of $\mathbf{t}$ onto $\text{span}(\mathbf{B})$\\
      Compute the $d_k$ distinct integers $c_1, \dots, c_{d_k}$ closest to $\langle \mathbf{v}, \tilde{\mathbf{b}}_k\rangle /  \langle \tilde{\mathbf{b}}_k, \tilde{\mathbf{b}}_k\rangle)$\\
      Return $\bigcup_{i \in \{1, \dots, d_k\}} (c_i \cdot \mathbf{b}_k + \text{GeneralizedNearestPlane}(\{\mathbf{b}_1, \dots, \mathbf{b}_{k-1}\}, (d_1, \dots, d_{k-1}), \mathbf{v} - c_i \cdot \mathbf{b}_k))$\\
    }
  }
\caption{Generalized Nearest Plane Algorithm \cite{LP11}}\label{alg:GeneralizedNearestPlane}
\end{algorithm2e}
Instead of choosing only the nearest plane in each iteration step, \cref{alg:GeneralizedNearestPlane} selects a variable amount $d_k$ of distinct planes in each step. As a consequence, the fundamental parallelipiped of the Gram-Schmidt basis is stretched in the direction of $\tilde{\mathbf{b}}_k$. The values of $\mathbf{d}$ should be chosen such that the covered area is approximately the same in each direction (i.e. by maximizing $\min_i(d_i \cdot \|\tilde{\mathbf{b}}_i\|)$). In particular this implies that the $d_k$ are larger for larger $k$ as the Gram-Schmidt vectors have a smaller length. % TODO combine with section above 
Compared to \cref{alg:babai} the runtime increases by a factor $\prod_{i \in \{1, \dots, d_k\}} d_i$, however, the recursion step can be fully parallelized.

It should be evident that a lower quality of the reduced input basis can be compensated for by increasing the values of $\mathbf{d}$. Hence we can adjust the input parameters for the lattice reduction and \cref{alg:GeneralizedNearestPlane} to minimize the runtime given a fixed required success probability. % TODO perhaps reformulate
% TODO: add exact success probability? 




\subsection{Primal-uSVP [ADPS16, BaiGal14]} % => AFG14, Kan87
% TODO: describe BKZ 2.0?

% TODO: taken from ADPS16, change
BKZ: reduce lattice basis using SVP oracle in smaller dimension $b$, known that number of calls to oracle polynomial
- enumeration algorithm as oracle: in super-exponential time
- sieve algorithms as oracle: exponential time but so far slower in practice for accesible dimensions $b\approx 130$

primal attack: construct unique-SVP instance from LWE instance % TODO check if u-SVP is defined
LWE instance $(\mathbf{A}, \mathbf{z} = \mathbf{A}^\intercal \mathbf{s} + \mathbf{e})$
construct lattice 
\begin{equation}
  \Lambda = \left\{ \mathbf{x} \in \mathbb{Z}^{m+n+1} \mid (\mathbf{A}^\intercal | -\mathbf{I}_m | -\mathbf{b})\mathbf{x} = \mathbf{0} \mod q \right\} 
\end{equation}
lattice has dimension $d=m+n+1$, volume $q^m$ % TODO show
and unique-SVP solution $\mathbf{v} = (\mathbf{s}, \mathbf{e}, 1)$ % TODO show

success condition:
- geometric series assumption known to be optimistic from attacker's point of view
=> finds basis with Gram-Schmidt norms $\|\tilde{mathbf{b}}_i\| = \delta^{d - 2i-1} \cdot \text{Vol}(\Lambda)^{1/d}$ and $\delta = ((\pi b)^{1/b} \cdot b/2\pi e)^{1/2(b-1)}$ % see APS15 and Chen13 thesis
unique short vector $\mathbf{v}$ is detected if projection of $\mathbf{v}$ onto span of last $b$ Gram-Schmidt vectors is shorter than $\tilde{mathbf{b}}_{d-b}$, norm of projection is expected to be $\gamma \sqrt{b}$ => attack successful iff $\gamma \sqrt{b} \leq \delta^{d - 2i-1} \cdot q^{m/d}$
% TODO define span, e.g. vector space spanned by a number of vectors
% TODO check if einheitsmatrix is defined

LWE as inhomogeneous-SIS (ISIS)

% FROM APS15:
As in \cref{sec:decoding}, we view the LWE$_{n, q, m, \chi}$ instance $(\mathbf{A}, \mathbf{z})$ as a BDD instance in the  $q$-ary lattice $\Lambda(\mathbf{A}^\intercal) = \{ \mathbf{y} \mid \exists \mathbf{x} \in \mathbb{Z}_q^n : \mathbf{y} = \mathbf{A}^\intercal \mathbf{x}  \mod q \}$ \cref{sec:lwe-bdd} generated by rows of LWE instance. The target vector is $\mathbf{z}$. % TODO add to lwe as BDD/combine

Recall the $\gamma$-uSVP problem. Given a lattice $\Lambda$ where $\lambda_2(\Lambda) > \gamma \lambda_1(\Lambda)$, we are asked to find shortest nonzero vector in $\Lambda$. 
In the primal attack, instead of directly solving BDD, we reduce BDD to uSVP, i.e., we reduce a BDD instance to a $\gamma$-uSVP instance. By solving $\gamma$-uSVP, we obtain a solution to BDD. 
To do this we apply Kannan's embedding technique \cite{Kan87}. % TODO: quote theorem? For any $\gamma \geq 1, there is a polynomial time Cook-reduction from BDD$_{1/(2\gamma)}$ to $\gamma$-uSVP.
Intuitively, Kannan's embedding creates a lattice with uSVP structure. We know that $\mathbf{A}^\intercal \mathbf{s}\mod q$ is the closest vector to the target $\mathbf{z} =\mathbf{A}^\intercal \mathbf{s} + \mathbf{e}^\intercal \mod q$ in $\Lambda(\mathbf{A}^\intercal)$. We now add a linearly indpenendent basis vector  $(\mathbf{z}, \mu)$ and append a zero coefficient to each basis vector of the original lattice (i.e. the rows of $\mathbf{A}$). Thereby, we ensure that the new lattice contains the vector $[-\mathbf{e}, -\mu]^\intercal$ as $[\mathbf{A} \mid \mathbf{0}]^\intercal \mathbf{s} - 1 \cdot [\mathbf{z}^\intercal, \mu] = [-\mathbf{e}, -\mu]^\intercal$. 

More formally, let $\mathbf{B}$ be a basis of $\Lambda(\mathbf{A}^\intercal)$ % TODO: how? is \Lambda(\mathbf{A}^\intercal) not q-ary? => is A^\intercal not a basis?
and an embedding factor $\mu = \text{dist}(\mathbf{z}, \Lambda(\mathbf{A}^\intercal)) = \| \mathbf{z} - \mathbf{s}\|$ where $\mathbf{s}$ is the secret vector of the LWE instance. A relatively close approximation of $\mu$ can be guessed in polynomial time (see \cite{LM09} for more details). We now embed $\Lambda(\mathbf{A}^\intercal)$ into $\Lambda(\mathbf{B}')$ with $\gamma$-uSVP structure as follows: % TODO: check if dist = \| \mathbf{z} - \mathbf{s}\| is correct, perhaps just a vector v such that equation is fulfilled???
\begin{equation}
  \mathbf{B}' = \begin{pmatrix}
    \mathbf{B} & \mathbf{z}\\
    \mathbf{0}^\intercal & \mu
  \end{pmatrix}
\end{equation}
If $\gamma \geq 1$ and $\mu < \frac{\lambda_1(\Lambda(\mathbf{B})}{2\gamma}$ (or equivalently, $(\Lambda(\mathbf{A}^\intercal), \mathbf{z})$ a BDD$_{1/(2\gamma)}$-instance), then $\Lambda(\mathbf{B}')$ contains a $\gamma$-unique shortest vector $\mathbf{z}' = \left[(\mathbf{A}^\intercal\mathbf{s} - \mathbf{z})^\intercal, -\mu\right]^\intercal = \left[-\mathbf{e}^\intercal, -\mu\right]^\intercal$. 
The statement can be proven by showing by contradiction that all vectors $\mathbf{v} \in \Lambda{\mathbf{B}'}$ that are independent of $\mathbf{z}'$ satisfy $\| \mathbf{v}\| \geq \lambda_1{\Lambda(\mathbf{B})}/\sqrt{2} > \sqrt{2}\gamma \mu = \gamma \|\mathbf{z}'\|$ (see Section 4 of \cite{LM09} for more details). Note that the reduction can be done in polynomial time (Theorem 4.1 in \cite{LM09}).% TODO possibly sketch the proof
Hence, from $\mathbf{z}'$ we can recover the error vector $\mathbf{e}$ and thereby the secret vector $\mathbf{s} = \mathbf{z} - \mathbf{e} \mod q$. 

A solution to $\gamma$-uSVP can be found by reducing it to $\kappa$-HSVP where $\gamma = \kappa^2$ \cite{APS15}. Various algorithms, in particular, lattice reduction algorithms, exist to solve $\kappa$-HSVP. If we are able to solve a linear number of $\kappa$-HSVP instances that correspond to a $\kappa^2$-approximate SVP instance, we can construct a solution the latter (see \cref{def:gammaSVP}, see Section 1.2.21 in \cite{Lov87} for more details). 
Consider any lattice with uSVP structure. In exactly one direction, that is, in the direction of its unique shortest vector, the lattice has vectors that are significantly smaller than in other directions. A lattice reduction algorithm that yields a sufficiently good output basis quality, therefore, must return some small vector in the desired direction. 
Let $\mathbf{v}$ be a solution to SVP$_\kappa^2$, i.e. $\|\mathbf{v}\| \leq \kappa^2 \lambda_1(\Lambda)$. All other vectors $\mathbf{w}\in \Lambda$ that are not multiples of a shortest vector have length $\|\mathbf{w}\| \geq \lambda_2(\Lambda) > \kappa^2\lambda_1(\Lambda)$. Thus, we obtain a solution to $\gamma$-uSVP and, as shown above, we can reconstruct the secret vector to solve LWE. 

% TODO: in practice: GN08

\subsection{Meet-in-the-Middle [AlbPlaSco15]}
\subsection{Arora-Ge [AroGe11,ACFP14]}




\section{Algorithms for Solving SIS}
\subsection{Dual Attack}
% TODO: checkout 
\subsubsection{MR variant [MR09]}
\subsubsection{RS variant [RS10]}
"concrete estimates of ``symmetric bit security'', concrete runtime estimates for various Hermite factors in random $q$-ary lattices"
permissive form of distiguishing attack in \cite{MR09}, adversarial advantage is about $2^{-72}$
\subsection{Combinatorial Attack [MR09]}


% TODO: somewhere section about sieving/enumeration
